{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Graph Classification with 🤗 Transformers**\n",
    "\n",
    "This notebook shows how to fine-tune the **Graphormer** model for Graph Classification on a dataset available on the hub. The idea is to add a randomly initialized classification head on top of a pre-trained encoder, and fine-tune the model altogether on a labeled dataset.\n",
    "\n",
    "Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory(OOM) errors. \n",
    "\n",
    "In this notebook, we'll fine-tune from the https://huggingface.co/clefourrier/pcqm4mv2-graphormer-base checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Graphormer on an graph classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to fine-tune the Graphormer model on [🤗 Transformers](https://github.com/huggingface/transformers) on a Graph Classification dataset.\n",
    "\n",
    "Given a graph, the goal is to predict its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CUDA_VISIBLE_DEVICES = \"0,1,2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a graph dataset from the Hub is very easy. Let's load the `VincentPai/encoded-MITRE` dataset, stored in the `VincentPai` repository. \n",
    "*To find other graph datasets, look for the \"Graph Machine Learning\" tag on the hub:  [here](https://huggingface.co/datasets?task_categories=task_categories:graph-ml&sort=downloads). You'll find social graphs, molecular datasets, some artificial ones, etc!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/workdir/home/euni/.cache/huggingface/datasets/VincentPai___json/VincentPai--encoded-MITRE-5bdfa67a52216983/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85662dc729a64a9ca82f9430075e9247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /workdir/home/euni/.cache/huggingface/datasets/VincentPai___json/VincentPai--encoded-MITRE-5bdfa67a52216983/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-866ffe29a73f78c8.arrow\n",
      "Loading cached shuffled indices for dataset at /workdir/home/euni/.cache/huggingface/datasets/VincentPai___json/VincentPai--encoded-MITRE-5bdfa67a52216983/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ca9a9a2bbcdb7ed1.arrow\n",
      "Loading cached shuffled indices for dataset at /workdir/home/euni/.cache/huggingface/datasets/VincentPai___json/VincentPai--encoded-MITRE-5bdfa67a52216983/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0233e4faf4c367f7.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "# dataset = load_dataset(\"VincentPai/for-graphormer-new2\")\n",
    "dataset = load_dataset(\"VincentPai/encoded-MITRE\")\n",
    "\n",
    "# rename the label to y to fit the format of the input of the Graphormer\n",
    "# dataset['train'] = dataset['train'].rename_column('label', 'y')\n",
    "\n",
    "dataset = dataset.shuffle(seed = 87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also load the Accuracy metric, which we'll use to evaluate our model both during and after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2647/3366162530.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dataset` object itself is a [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key per split (in this case, \"train\", \"validation\" and \"test\" splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['y', 'num_nodes', 'node_feat', 'edge_attr', 'edge_index'],\n",
      "        num_rows: 2959563\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['y', 'num_nodes', 'node_feat', 'edge_attr', 'edge_index'],\n",
      "        num_rows: 986521\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['y', 'num_nodes', 'node_feat', 'edge_attr', 'edge_index'],\n",
      "        num_rows: 986521\n",
      "    })\n",
      "})\n",
      "{'y': [0], 'num_nodes': 3, 'node_feat': [[368205], [6], [585037]], 'edge_attr': [[0], [0]], 'edge_index': [[0, 1], [1, 2]]}\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "print(dataset, dataset[\"train\"][0], sep = '\\n')\n",
    "# print(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the graph using networkx and pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [1, 2]]\n",
      "{'y': [0], 'num_nodes': 3, 'node_feat': [[368205], [6], [585037]], 'edge_attr': [[0], [0]], 'edge_index': [[0, 1], [1, 2]]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# We want to plot the first train graph\n",
    "graph = dataset[\"train\"][0] # would be a json here\n",
    "edges = graph[\"edge_index\"]\n",
    "\n",
    "print(graph[\"edge_index\"])\n",
    "print(graph)\n",
    "\n",
    "num_edges = len(edges[0])\n",
    "num_nodes = graph[\"num_nodes\"]\n",
    "\n",
    "G = nx.Graph()\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(num_nodes))\n",
    "G.add_edges_from([(edges[0][i], edges[1][i]) for i in range(num_edges)])\n",
    "\n",
    "# Plot\n",
    "nx.draw(G, with_labels=True)\n",
    "\n",
    "\n",
    "# print(graph[\"node_feat\"], type(graph[\"node_feat\"]))\n",
    "# graph[\"node_feat\"].append(0)\n",
    "# print(graph[\"node_feat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph transformer frameworks usually apply specific preprocessing to their datasets to generate added features and properties which help the underlying learning task (classification in our case).\n",
    "\n",
    "Here, we use Graphormer's default preprocessing, which generates in/out degree information, the shortest path between node matrices, and other properties of interest for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /workdir/home/euni/.cache/huggingface/datasets/VincentPai___json/VincentPai--encoded-MITRE-5bdfa67a52216983/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-647ec9f1adc9e746.arrow\n",
      "Loading cached processed dataset at /workdir/home/euni/.cache/huggingface/datasets/VincentPai___json/VincentPai--encoded-MITRE-5bdfa67a52216983/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dfd6f8ef02371a10.arrow\n",
      "Loading cached processed dataset at /workdir/home/euni/.cache/huggingface/datasets/VincentPai___json/VincentPai--encoded-MITRE-5bdfa67a52216983/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-96b9d49c74e7a073.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator\n",
    "dataset_processed = dataset.map(preprocess_item, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['y', 'num_nodes', 'node_feat', 'edge_attr', 'edge_index', 'input_nodes', 'attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'out_degree', 'input_edges', 'labels'],\n",
       "        num_rows: 2959563\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['y', 'num_nodes', 'node_feat', 'edge_attr', 'edge_index', 'input_nodes', 'attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'out_degree', 'input_edges', 'labels'],\n",
       "        num_rows: 986521\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['y', 'num_nodes', 'node_feat', 'edge_attr', 'edge_index', 'input_nodes', 'attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'out_degree', 'input_edges', 'labels'],\n",
       "        num_rows: 986521\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "\n",
    "train_ds = dataset_processed['train']\n",
    "val_ds = dataset_processed['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Part for the data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['y', 'num_nodes', 'node_feat', 'edge_attr', 'edge_index', 'input_nodes', 'attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'out_degree', 'input_edges', 'labels'])\n",
      "3\n",
      "[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]\n",
      "[[368205], [6], [585037]]\n",
      "[[368208], [9], [585040]]\n",
      "3\n",
      "input_nodes shape: (3, 1)\n",
      "attn_bias shape: (4, 4)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nattn_bias = np.zeros([num_nodes + 1, num_nodes + 1], dtype=np.single)  # with graph token -> 4x4 in our case\\nitem[\"attn_bias\"] = attn_bias # in our case, it will be 4x4 tensor with element 0\\n\\nbatch[\"attn_bias\"] = torch.ze ros(batch_size, max_node_num + 1, max_node_num + 1, dtype=torch.float)\\nmax_node_num = max(len(i[\"input_nodes\"]) for i in features)\\n\\nerror : batch[\"attn_bias\"][ix, : f[\"attn_bias\"].shape[0], : f[\"attn_bias\"].shape[1]] = f[\"attn_bias\"] \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(train_ds[0].keys())\n",
    "\n",
    "# print(train_ds[0]['attn_bias'].shape)\n",
    "\n",
    "# attn_bias is 4x4\n",
    "print(train_ds[0]['num_nodes'])\n",
    "print(train_ds[0]['attn_bias'])\n",
    "print(train_ds[0]['node_feat'])\n",
    "print(train_ds[0]['input_nodes'], len(train_ds[0]['input_nodes']), sep='\\n')\n",
    "\n",
    "my_array = np.array(train_ds[0]['input_nodes'])\n",
    "print('input_nodes shape:', my_array.shape)\n",
    "\n",
    "my_array = np.array(train_ds[0]['attn_bias'])\n",
    "print('attn_bias shape:', my_array.shape, end='\\n\\n')\n",
    "\n",
    "''' \n",
    "attn_bias = np.zeros([num_nodes + 1, num_nodes + 1], dtype=np.single)  # with graph token -> 4x4 in our case\n",
    "item[\"attn_bias\"] = attn_bias # in our case, it will be 4x4 tensor with element 0\n",
    "\n",
    "batch[\"attn_bias\"] = torch.ze ros(batch_size, max_node_num + 1, max_node_num + 1, dtype=torch.float)\n",
    "max_node_num = max(len(i[\"input_nodes\"]) for i in features)\n",
    "\n",
    "error : batch[\"attn_bias\"][ix, : f[\"attn_bias\"].shape[0], : f[\"attn_bias\"].shape[1]] = f[\"attn_bias\"] \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(3, 1)\n",
      "24\n",
      "[[445528]]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_single_emb(x, offset: int = 512):\n",
    "    feature_num = x.shape[1] if len(x.shape) > 1 else 1\n",
    "    feature_offset = 1 + np.arange(0, feature_num * offset, offset, dtype=np.int64)\n",
    "    x = x + feature_offset\n",
    "    return x\n",
    "\n",
    "\n",
    "node_feat = [[445528, 24, 740662]]\n",
    "node_feature = np.asarray(node_feat, dtype=np.int64)\n",
    "input_nodes = convert_to_single_emb(node_feature) + 1\n",
    "print(input_nodes.shape)\n",
    "\n",
    "node_feat = [[445528], [24], [740662]]\n",
    "node_feature = np.asarray(node_feat, dtype=np.int64)\n",
    "input_nodes = convert_to_single_emb(node_feature) + 1\n",
    "print(input_nodes.shape)\n",
    "\n",
    "node_feat_2 = [[445528, 24, 740662]] \n",
    "print(node_feat_2[0][1])\n",
    "\n",
    "# 將node_feat轉換為3x1的二維陣列\n",
    "node_feat_2d_array = [[item[0]] for item in node_feat_2]\n",
    "\n",
    "print(node_feat_2d_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `from_pretrained` method on our model downloads and caches the weights for us. As the number of classes (for prediction) is dataset dependent, we pass the new `num_classes` as well as `ignore_mismatched_sizes` alongside the `model_checkpoint`. This makes sure a custom classification head is created, specific to our task, hence likely different from the original decoder head. \n",
    "\n",
    "(When using a pretrained model, you must make sure the embeddings of your data have the same shape as the ones used to pretrain your model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GraphormerForGraphClassification were not initialized from the model checkpoint at clefourrier/graphormer-base-pcqm4mv2 and are newly initialized because the shapes did not match:\n",
      "- classifier.classifier.weight: found shape torch.Size([1, 768]) in the checkpoint and torch.Size([168, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GraphormerForGraphClassification\n",
    "\n",
    "model_checkpoint = \"clefourrier/graphormer-base-pcqm4mv2\" # pre-trained model from which to fine-tune\n",
    "\n",
    "model = GraphormerForGraphClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "\n",
    "    # We have 167 attack patterns and 1 benign\n",
    "    num_classes=168, \n",
    "\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning is telling us we are throwing away some weights (the weights and bias of the `classifier` layer) and randomly initializing some other (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate a `Trainer`, we will need to define the training configuration and the evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model.\n",
    "\n",
    "For graph datasets, it is particularly important to play around with batch sizes and gradient accumulation steps to train on enough samples while avoiding out-of-memory errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"graph-classification\",\n",
    "    logging_dir=\"graph-classification\",\n",
    "    \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "\n",
    "    auto_find_batch_size=False, # batch size can be changed automatically to prevent OOMs\n",
    "    gradient_accumulation_steps=10,\n",
    "    dataloader_num_workers=4, \n",
    "\n",
    "    num_train_epochs=5,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "\n",
    "    disable_tqdm=False,  # show the tqdm bar\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Trainer` for graph classification, it is important to pass the specific data collator for the given graph dataset, which will convert individual graphs to batches for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class PrintLossCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f\"Epoch: {state.epoch}, Step: {state.global_step}, Loss: {state.log_history[-1]['loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {\"accuracy\": accuracy_score(p.label_ids, preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "\n",
    "    # data collator is important in our task\n",
    "    data_collator=GraphormerDataCollator(),\n",
    "    callbacks=[PrintLossCallback()],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir/home/euni/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now upload the result of the training to the Hub with the following:\n",
    "- Need to login first(add some code in the front of the script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
