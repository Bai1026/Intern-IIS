{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDatasetError",
     "evalue": "The dataset repository at 'OGB/ogbg-molhiv' doesn't contain any data files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDatasetError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workdir/home/bai/graphormer_test.ipynb 儲存格 1\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496e7465726e5f494953227d/workdir/home/bai/graphormer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m download_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/workdir/home/bai/data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496e7465726e5f494953227d/workdir/home/bai/graphormer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# dataset = load_dataset(url)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496e7465726e5f494953227d/workdir/home/bai/graphormer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(url, data_dir\u001b[39m=\u001b[39;49mdownload_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496e7465726e5f494953227d/workdir/home/bai/graphormer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# For the train set (replace by valid or test as needed)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496e7465726e5f494953227d/workdir/home/bai/graphormer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m ogbg_molhiv_pg_list \u001b[39m=\u001b[39m [Data(graph) \u001b[39mfor\u001b[39;00m graph \u001b[39min\u001b[39;00m dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/load.py:1785\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1781\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1782\u001b[0m )\n\u001b[1;32m   1784\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1785\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1786\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1787\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1788\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1789\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1790\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1791\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1792\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1793\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1794\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1795\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1796\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1797\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1798\u001b[0m )\n\u001b[1;32m   1800\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/load.py:1514\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1513\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1514\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1515\u001b[0m     path,\n\u001b[1;32m   1516\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1517\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1518\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1519\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1520\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1521\u001b[0m )\n\u001b[1;32m   1523\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/load.py:1225\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1224\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e1, EmptyDatasetError):\n\u001b[0;32m-> 1225\u001b[0m     \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e1, \u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1227\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1228\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1229\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m on the Hugging Face Hub either: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e1)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1230\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/load.py:1208\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1201\u001b[0m             path,\n\u001b[1;32m   1202\u001b[0m             revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1205\u001b[0m             dynamic_modules_path\u001b[39m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1206\u001b[0m         )\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m   1207\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1208\u001b[0m         \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1209\u001b[0m             path,\n\u001b[1;32m   1210\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1211\u001b[0m             data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1212\u001b[0m             data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1213\u001b[0m             download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1214\u001b[0m             download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1215\u001b[0m         )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m   1216\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[1;32m   1217\u001b[0m     \u001b[39mException\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m ) \u001b[39mas\u001b[39;00m e1:  \u001b[39m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/load.py:780\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_module\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DatasetModule:\n\u001b[1;32m    771\u001b[0m     hfh_dataset_info \u001b[39m=\u001b[39m HfApi(config\u001b[39m.\u001b[39mHF_ENDPOINT)\u001b[39m.\u001b[39mdataset_info(\n\u001b[1;32m    772\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    773\u001b[0m         revision\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrevision,\n\u001b[1;32m    774\u001b[0m         token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_config\u001b[39m.\u001b[39muse_auth_token,\n\u001b[1;32m    775\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39m100.0\u001b[39m,\n\u001b[1;32m    776\u001b[0m     )\n\u001b[1;32m    777\u001b[0m     patterns \u001b[39m=\u001b[39m (\n\u001b[1;32m    778\u001b[0m         sanitize_patterns(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files)\n\u001b[1;32m    779\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 780\u001b[0m         \u001b[39melse\u001b[39;00m get_data_patterns_in_dataset_repository(hfh_dataset_info, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_dir)\n\u001b[1;32m    781\u001b[0m     )\n\u001b[1;32m    782\u001b[0m     data_files \u001b[39m=\u001b[39m DataFilesDict\u001b[39m.\u001b[39mfrom_hf_repo(\n\u001b[1;32m    783\u001b[0m         patterns,\n\u001b[1;32m    784\u001b[0m         dataset_info\u001b[39m=\u001b[39mhfh_dataset_info,\n\u001b[1;32m    785\u001b[0m         base_path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir,\n\u001b[1;32m    786\u001b[0m         allowed_extensions\u001b[39m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    787\u001b[0m     )\n\u001b[1;32m    788\u001b[0m     split_modules \u001b[39m=\u001b[39m {\n\u001b[1;32m    789\u001b[0m         split: infer_module_for_data_files(data_files_list, use_auth_token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_config\u001b[39m.\u001b[39muse_auth_token)\n\u001b[1;32m    790\u001b[0m         \u001b[39mfor\u001b[39;00m split, data_files_list \u001b[39min\u001b[39;00m data_files\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    791\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/data_files.py:661\u001b[0m, in \u001b[0;36mget_data_patterns_in_dataset_repository\u001b[0;34m(dataset_info, base_path)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_data_files_patterns(resolver)\n\u001b[1;32m    660\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     \u001b[39mraise\u001b[39;00m EmptyDatasetError(\n\u001b[1;32m    662\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe dataset repository at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdataset_info\u001b[39m.\u001b[39mid\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain any data files\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    663\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mEmptyDatasetError\u001b[0m: The dataset repository at 'OGB/ogbg-molhiv' doesn't contain any data files"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "url = \"graphs-datasets/ogbg-molhiv\"\n",
    "# download_path = \"/workdir/home/bai\"\n",
    "download_path = \"/workdir/home/bai/data\"\n",
    "\n",
    "# dataset = load_dataset(url)\n",
    "dataset = load_dataset(url, data_dir=download_path) #data_dir is not where we put our data in ==\n",
    "# For the train set (replace by valid or test as needed)\n",
    "ogbg_molhiv_pg_list = [Data(graph) for graph in dataset[\"train\"]]\n",
    "ogbg_molhiv_pg = DataLoader(ogbg_molhiv_pg_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"OGB/ogbg-molhiv\")\n",
    "dataset = dataset.shuffle(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 从内存中获取样本\n",
    "        graph = self.dataset[idx]\n",
    "        edges = graph[\"edge_index\"]\n",
    "        num_edges = len(edges[0])\n",
    "        num_nodes = graph[\"num_nodes\"]\n",
    "\n",
    "        # Conversion to networkx format\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(num_nodes))\n",
    "        G.add_edges_from([(edges[0][i], edges[1][i]) for i in range(num_edges)])\n",
    "\n",
    "        # 在此处可以根据需要进行进一步的预处理和特征提取\n",
    "        # 这里只返回图数据，您可以自定义返回的内容\n",
    "\n",
    "        return G\n",
    "\n",
    "# 创建自定义数据集对象\n",
    "my_dataset = MyDataset(dataset)\n",
    "\n",
    "# 创建 DataLoader 对象\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练和推理函数\n",
    "def train(model, data):\n",
    "    # 在此处编写训练逻辑\n",
    "    pass\n",
    "\n",
    "def evaluate(model, data):\n",
    "    # 在此处编写推理逻辑\n",
    "    pass\n",
    "\n",
    "# 创建模型\n",
    "model = GraphormerForGraphClassification.from_pretrained(\n",
    "    \"clefourrier/pcqm4mv2_graphormer_base\",\n",
    "    num_classes=2, # num_classes for the downstream task \n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# 在 DataLoader 上进行训练和推理\n",
    "for batch in data_loader:\n",
    "    train(model, batch)\n",
    "    evaluate(model, batch)\n",
    "\n",
    "# 如果需要保存模型\n",
    "model.save_pretrained(\"path/to/save/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
