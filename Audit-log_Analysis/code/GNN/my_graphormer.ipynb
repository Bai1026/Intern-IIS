{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Graph Classification with ðŸ¤— Transformers**\n",
    "\n",
    "This notebook shows how to fine-tune the Graphormer model for Graph Classification on a dataset available on the hub. The idea is to add a randomly initialized classification head on top of a pre-trained encoder, and fine-tune the model altogether on a labeled dataset.\n",
    "\n",
    "Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those two parameters, then the rest of the notebook should run smoothly.\n",
    "\n",
    "In this notebook, we'll fine-tune from the https://huggingface.co/clefourrier/pcqm4mv2-graphormer-base checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Graphormer on an graph classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to fine-tune the Graphormer model on [ðŸ¤— Transformers](https://github.com/huggingface/transformers) on a Graph Classification dataset.\n",
    "\n",
    "Given a graph, the goal is to predict its class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a graph dataset from the Hub is very easy. Let's load the `ogbg-molhiv` dataset, stored in the `OGB` repository. \n",
    "*To find other graph datasets, look for the \"Graph Machine Learning\" tag on the hub:  [here](https://huggingface.co/datasets?task_categories=task_categories:graph-ml&sort=downloads). You'll find social graphs, molecular datasets, some artificial ones, etc!*\n",
    "\n",
    "This dataset contains a collection of molecules (from MoleculeNet), and the goal is to predict if they to inhibit HIV or not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"VincentPai/for-graphormer-v4\")\n",
    "\n",
    "# rename the label to y to fit the format of the input of the Graphormer\n",
    "dataset['train'] = dataset['train'].rename_column('label', 'y')\n",
    "\n",
    "dataset = dataset.shuffle(seed = 87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also load the Accuracy metric, which we'll use to evaluate our model both during and after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dataset` object itself is a [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key per split (in this case, \"train\", \"validation\" and \"test\" splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the graph using networkx and pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# We want to plot the first train graph\n",
    "graph = dataset[\"train\"][0] # would be a json here\n",
    "\n",
    "# print(graph[\"node_feat\"], type(graph[\"node_feat\"]))\n",
    "# graph[\"node_feat\"].append(0)\n",
    "# print(graph[\"node_feat\"])\n",
    "\n",
    "edges = graph[\"edge_index\"]\n",
    "print(graph[\"edge_index\"])\n",
    "\n",
    "num_edges = len(edges[0])\n",
    "num_nodes = graph[\"num_nodes\"]\n",
    "\n",
    "# print(type(dataset), dataset[\"label\"])\n",
    "\n",
    "# print(graph[\"num_nodes\"], type(graph[\"num_nodes\"]))\n",
    "# print(num_nodes)\n",
    "# # num_nodes = graph[\"num_nodes\"]\n",
    "# num_nodes = 100\n",
    "# graph[\"num_nodes\"] = 5\n",
    "# Conversion to networkx format\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(num_nodes))\n",
    "G.add_edges_from([(edges[0][i], edges[1][i]) for i in range(num_edges)])\n",
    "\n",
    "# Plot\n",
    "# nx.draw(G)\n",
    "nx.draw(G, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph transformer frameworks usually apply specific preprocessing to their datasets to generate added features and properties which help the underlying learning task (classification in our case).\n",
    "\n",
    "Here, we use Graphormer's default preprocessing, which generates in/out degree information, the shortest path between node matrices, and other properties of interest for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator\n",
    "\n",
    "dataset_processed = dataset.map(preprocess_item, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "\n",
    "train_ds = dataset_processed['train']\n",
    "val_ds = dataset_processed['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access an element to look at all the features we've added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `from_pretrained` method on our model downloads and caches the weights for us. As the number of classes (for prediction) is dataset dependent, we pass the new `num_classes` as well as `ignore_mismatched_sizes` alongside the `model_checkpoint`. This makes sure a custom classification head is created, specific to our task, hence likely different from the original decoder head. \n",
    "\n",
    "(When using a pretrained model, you must make sure the embeddings of your data have the same shape as the ones used to pretrain your model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GraphormerForGraphClassification\n",
    "\n",
    "model_checkpoint = \"clefourrier/graphormer-base-pcqm4mv2\" # pre-trained model from which to fine-tune\n",
    "\n",
    "model = GraphormerForGraphClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_classes=2,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning is telling us we are throwing away some weights (the weights and bias of the `classifier` layer) and randomly initializing some other (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate a `Trainer`, we will need to define the training configuration and the evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model.\n",
    "\n",
    "For graph datasets, it is particularly important to play around with batch sizes and gradient accumulation steps to train on enough samples while avoiding out-of-memory errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    \"graph-classification\",\n",
    "    logging_dir=\"graph-classification\",\n",
    "    \n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "\n",
    "    auto_find_batch_size=True, # batch size can be changed automatically to prevent OOMs\n",
    "    gradient_accumulation_steps=10,\n",
    "    dataloader_num_workers=4, \n",
    "\n",
    "    num_train_epochs=2,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Trainer` for graph classification, it is important to pass the specific data collator for the given graph dataset, which will convert individual graphs to batches for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    # eval_dataset=val_ds,\n",
    "    data_collator=GraphormerDataCollator()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cna now train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now upload the result of the training to the Hub with the following:\n",
    "- Need to login first(add some code in the front of the script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
