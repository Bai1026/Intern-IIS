{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load embedding function\n",
    "def load_embedding(input_embedding_name, model):\n",
    "    if model.startswith('trans'):\n",
    "        with open(input_embedding_name) as f:\n",
    "            data = json.load(f)\n",
    "        ent_embeddings = np.array(data['ent_embeddings.weight'])\n",
    "        rel_embeddings = np.array(data['rel_embeddings.weight'])\n",
    "        return ent_embeddings, rel_embeddings\n",
    "    \n",
    "    elif model == 'secureBERT':\n",
    "        ent_embeddings = np.empty((0, 768), dtype=np.float32)\n",
    "        for filename in sorted(os.listdir(input_embedding_name)):\n",
    "            print(filename)\n",
    "\n",
    "            if not filename.startswith('embeddings_chunk'):\n",
    "                continue\n",
    "\n",
    "            embedding = np.load(f'{input_embedding_name}/{filename}')\n",
    "\n",
    "            print(ent_embeddings.shape, embedding.shape)\n",
    "\n",
    "            ent_embeddings = np.concatenate((ent_embeddings, embedding), axis=0)\n",
    "            print(filename, ent_embeddings.shape)\n",
    "\n",
    "        print(f'Reducing entity embedding to ({DIM},)')\n",
    "        print(ent_embeddings.shape, '->', end=' ')\n",
    "        \n",
    "        pca = PCA(n_components=DIM)\n",
    "        ent_embeddings = pca.fit_transform(ent_embeddings)\n",
    "        print(ent_embeddings.shape)\n",
    "\n",
    "        rel_embeddings = np.load(f'{input_embedding_name}/relation.npy')\n",
    "        print(f'Reducing relation embedding to ({len(rel_embeddings)},)')\n",
    "        print(rel_embeddings.shape, '->', end=' ')\n",
    "        pca = PCA(n_components=len(rel_embeddings))\n",
    "        rel_embeddings = pca.fit_transform(rel_embeddings)\n",
    "        print(rel_embeddings.shape)\n",
    "        return ent_embeddings, rel_embeddings\n",
    "    else:\n",
    "        print('Error!!')\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c3f6e2e14c4f9096ff06ddfc5d4e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff286bcb34c46b39df9a0e31141083e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file name: ../data_new/exp3/graph/secureBERT_150_embedded.jsonl\n",
      ".ipynb_checkpoints\n",
      "embeddings_chunk_0.npy\n",
      "(0, 768) (160000, 768)\n",
      "embeddings_chunk_0.npy (160000, 768)\n",
      "embeddings_chunk_1.npy\n",
      "(160000, 768) (160000, 768)\n",
      "embeddings_chunk_1.npy (320000, 768)\n",
      "embeddings_chunk_2.npy\n",
      "(320000, 768) (160000, 768)\n",
      "embeddings_chunk_2.npy (480000, 768)\n",
      "embeddings_chunk_3.npy\n",
      "(480000, 768) (160000, 768)\n",
      "embeddings_chunk_3.npy (640000, 768)\n",
      "embeddings_chunk_4.npy\n",
      "(640000, 768) (160000, 768)\n",
      "embeddings_chunk_4.npy (800000, 768)\n",
      "embeddings_chunk_5.npy\n",
      "(800000, 768) (160000, 768)\n",
      "embeddings_chunk_5.npy (960000, 768)\n",
      "embeddings_chunk_6.npy\n",
      "(960000, 768) (160000, 768)\n",
      "embeddings_chunk_6.npy (1120000, 768)\n",
      "embeddings_chunk_7.npy\n",
      "(1120000, 768) (51204, 768)\n",
      "embeddings_chunk_7.npy (1171204, 768)\n",
      "relation.npy\n",
      "Reducing entity embedding to (150,)\n",
      "(1171204, 768) -> (1171204, 150)\n",
      "Reducing relation embedding to (26,)\n",
      "(26, 768) -> (26, 26)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c74ff23ed804de7b6ab83bad89b0005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_files = [\"../data_new/embedding/secureBERT\"]\n",
    "model = 'secureBERT'\n",
    "\n",
    "# 输入文件列表\n",
    "# input_filenames = [\"../data_new/graph/benign/graph_benign.jsonl\"]\n",
    "# input_filenames = [\"../data_new/graph/without_benign/graph_without_benign.jsonl\"]\n",
    "input_filenames = [\"../data_new/exp3/graph/graph_exp3.jsonl\"]\n",
    "\n",
    "# for i in tqdm(range(3)):\n",
    "#     DIM = 150 - 50*i\n",
    "DIM = 150\n",
    "\n",
    "for input_filename in tqdm(input_filenames):\n",
    "    print(\"Start!\")\n",
    "    base, ext = os.path.splitext(input_filename)\n",
    "\n",
    "    with open(input_filename, \"r\") as f:\n",
    "        input_data = list(f)\n",
    "\n",
    "    for embedding_file in tqdm(embedding_files):\n",
    "        output_filename = f\"../data_new/exp3/graph/secureBERT_{DIM}_embedded.jsonl\"\n",
    "\n",
    "        print(f\"output file name: {output_filename}\")\n",
    "\n",
    "        with open(output_filename, \"w\") as out_file:\n",
    "            model = embedding_file.split('/')[-1].split('_')[0]\n",
    "            ent_embeddings, rel_embeddings = load_embedding(embedding_file, model)\n",
    "            # ...\n",
    "\n",
    "            for line, data in tqdm(zip(input_data, input_data)):\n",
    "                data = json.loads(data.strip())\n",
    "\n",
    "                # Replace node_feat and edge_attr with embeddings\n",
    "                data[\"node_feat\"] = [ent_embeddings[node_id].tolist() if model == 'secureBERT' else ent_embeddings[node_id] for node_id in data[\"node_feat\"]]\n",
    "                data[\"edge_attr\"] = [rel_embeddings[edge_id].tolist() for edge_id in data[\"edge_attr\"]]\n",
    "\n",
    "                # Convert the data back to a JSON string and write to the output file\n",
    "                out_file.write(json.dumps(data) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine 2 jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2657470da67416d820e16162bd90d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_new/graph/with_benign/secureBERT_150_embedded.jsonl\n",
      "../data_new/graph/with_benign/secureBERT_100_embedded.jsonl\n",
      "../data_new/graph/with_benign/secureBERT_50_embedded.jsonl\n"
     ]
    }
   ],
   "source": [
    "# files = ['transE_50', 'transE_100', 'transE_150', 'transH_50', 'transH_100', 'transH_150', 'transR_50',\n",
    "#          'secureBERT_250', 'secureBERT_150', 'secureBERT_100', 'secureBERT_50']\n",
    "files = ['secureBERT_150', 'secureBERT_100', 'secureBERT_50']\n",
    "\n",
    "for file in tqdm(files):\n",
    "    file1 = f\"../data_new/graph/benign/{file}_embedded.jsonl\"\n",
    "    data1 = []\n",
    "\n",
    "    with open(file1, 'r') as f:\n",
    "        for line in f:\n",
    "            data1.append(json.loads(line))\n",
    "\n",
    "    file2 = f\"../data_new/graph/without_benign/{file}_embedded.jsonl\"\n",
    "    data2 = []\n",
    "\n",
    "    with open(file2, 'r') as f:\n",
    "        for line in f:\n",
    "            data2.append(json.loads(line))\n",
    "\n",
    "    combined_data = data1 + data2\n",
    "\n",
    "    output_file = f\"../data_new/graph/with_benign/{file}_embedded.jsonl\"\n",
    "    print(output_file)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in combined_data:\n",
    "            f.write(json.dumps(item) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
