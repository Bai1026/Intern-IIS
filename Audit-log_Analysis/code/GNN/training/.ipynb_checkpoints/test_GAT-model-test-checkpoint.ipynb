{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of GAT\n",
    "- use DGL\n",
    "- test some model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import json\n",
    "import torch\n",
    "import torch as th\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from dgl.nn import GraphConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the GPU and assign the GPU by the best memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "\n",
    "def get_free_gpu():\n",
    "    try:\n",
    "        # Run nvidia-smi command to get GPU details\n",
    "        _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "        command = \"nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader\"\n",
    "        memory_free_info = _output_to_list(subprocess.check_output(command.split())) \n",
    "        memory_free_values = [int(x) for i, x in enumerate(memory_free_info)]\n",
    "        \n",
    "        # Get the GPU with the maximum free memory\n",
    "        best_gpu_id = memory_free_values.index(max(memory_free_values))\n",
    "        return best_gpu_id\n",
    "    except:\n",
    "        # If any exception occurs, default to GPU 0 (this handles cases where nvidia-smi isn't installed)\n",
    "        return 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Get the best GPU ID based on free memory and set it\n",
    "    best_gpu_id = get_free_gpu()\n",
    "    device = torch.device(f\"cuda:{best_gpu_id}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#fix seed\n",
    "def same_seeds(seed = 8787):\n",
    "    torch.manual_seed(seed)\n",
    "    # random.seed(seed) \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_list, device):\n",
    "        self.data_list = data_list\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "\n",
    "        g = dgl.graph((th.tensor(data[\"edge_index\"][0]), th.tensor(data[\"edge_index\"][1])), num_nodes=data[\"num_nodes\"]).to(self.device)\n",
    "\n",
    "        g.ndata['feat'] = th.tensor(data[\"node_feat\"]).to(self.device)\n",
    "        g.edata['feat'] = th.tensor(data[\"edge_attr\"]).to(self.device)  # Add edge features to graph\n",
    "\n",
    "        return g, th.tensor(data[\"label\"]).to(self.device)\n",
    "\n",
    "\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data_processing/dgl/data/test/repeated_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130000it [00:29, 4419.04it/s]\n",
      " 33%|███▎      | 1/3 [00:29<00:58, 29.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data_processing/dgl/data/test/repeated_valid.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:00, 3963.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data_processing/dgl/data/test/repeated_test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:00, 2263.66it/s]\n",
      "100%|██████████| 3/3 [00:29<00:00,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = ['train', 'valid', 'test']\n",
    "dataloaders = {}\n",
    "\n",
    "for dataset_name in tqdm(datasets):\n",
    "#     file_path = f\"../data/training_data/repeated_{dataset_name}.jsonl\"\n",
    "#     file_path = f\"../data/test_10(500times)/repeated_{dataset_name}.jsonl\"\n",
    "    file_path = f\"../../data_processing/dgl/data/test/repeated_{dataset_name}.jsonl\"\n",
    "    \n",
    "    print(file_path)\n",
    "    with open(file_path) as f:\n",
    "#         data_list = [json.loads(line) for line in f]\n",
    "        data_list = [json.loads(line) for line in tqdm(f, position=0, leave=True)]\n",
    "    \n",
    "    dataset = GraphDataset(data_list, device)\n",
    "    dataloaders[dataset_name] = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=collate)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "- Try teh model with 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, dropout_prob=0.25):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        # do not check the zero in_degree since we have all the complete graph\n",
    "        self.layer1 = GATConv(in_dim, hidden_dim, num_heads=num_heads, activation=F.relu, allow_zero_in_degree=True)\n",
    "        self.layer2 = GATConv(hidden_dim * num_heads, hidden_dim, num_heads=num_heads, allow_zero_in_degree=True)\n",
    "        self.layer3 = GATConv(hidden_dim * num_heads, out_dim, num_heads=num_heads, allow_zero_in_degree=True)\n",
    "\n",
    "#         self.layer2 = GATConv(hidden_dim * num_heads, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n",
    "#         self.layer3 = GATConv(hidden_dim * num_heads, out_dim, num_heads=1, allow_zero_in_degree=True)\n",
    "         \n",
    "        # Adding Batch Normalization after each GAT layer\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_dim * num_heads)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_dim * num_heads)\n",
    "#         self.batchnorm3 = nn.BatchNorm1d(out_dim) # there's no need to use BN3\n",
    "        \n",
    "        # Adding Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # Layer 1\n",
    "        h1 = self.layer1(g, h)\n",
    "        h1 = h1.view(h1.shape[0], -1)\n",
    "        h1 = F.relu(h1)\n",
    "        h1 = self.dropout(h1)\n",
    "        \n",
    "        # Layer 2\n",
    "        h2 = self.layer2(g, h1)\n",
    "        h2 = h2.view(h2.shape[0], -1)\n",
    "        h2 = F.relu(h2)\n",
    "        h2 = self.dropout(h2)\n",
    "\n",
    "        # Layer 3\n",
    "        h3 = self.layer3(g, h2).squeeze(1)\n",
    "        h3 = self.dropout(h3)\n",
    "        \n",
    "        '''\n",
    "        問題出現在 h3 = self.layer3(g, h2).squeeze(1)。\n",
    "        在這裡，你應該得到一個形狀為 [N, num_heads, out_dim] 的tensor，但你使用了 squeeze(1)，\n",
    "        如果 num_heads 是 1，你會得到 [N, out_dim]，這樣是沒問題的。\n",
    "        但如果 num_heads 不是 1，那麼squeeze操作不會更改tensor的形狀，結果仍然是 [N, num_heads, out_dim]。\n",
    "        因此，對這個tensor使用 batch normalization 會導致維度不匹配。\n",
    "        '''\n",
    "        # output layer so not need the BN\n",
    "        # 不使用BN: GAT本身已經有注意力機制，所以BN不一定是必需的，尤其是在輸出層。\n",
    "        # h3 = self.batchnorm3(h3)\n",
    "        \n",
    "\n",
    "\n",
    "        # Aggregate\n",
    "        g.ndata['h_out'] = h3\n",
    "        h_agg = dgl.mean_nodes(g, feat='h_out')\n",
    "        return h_agg\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Forward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(data, model, criterion, device, count=1):\n",
    "    \"\"\"Forward a batch through the model.\"\"\"\n",
    "    batched_g, labels = data\n",
    "#     print(batch_g)\n",
    "    batched_g = batched_g.to(device)\n",
    "    \n",
    "    labels = labels.to(device)\n",
    "    logits = model(batched_g, batched_g.ndata['feat'].float()) # for GAT\n",
    "    logits = logits.mean(dim=1)\n",
    "#     print(logits)\n",
    "    \n",
    "    loss = criterion(logits, labels)\n",
    "#     print(batched_g.ndata['feat'].dtype)\n",
    "#     print(\"Logits shape:\", logits.shape)  # Expected: (batch_size, 168)\n",
    "#     print(\"Labels shape:\", labels.shape)  # Expected: (batch_size)\n",
    "\n",
    "    # Get the class id with the highest probability.\n",
    "    preds = logits.argmax(1)\n",
    "    \n",
    "    # Compute accuracy.\n",
    "    accuracy = torch.mean((preds == labels).float())\n",
    "\n",
    "#     return loss, accuracy\n",
    "    return loss, accuracy, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fix the seed and save the model.state_dict that contains the initial weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8787\n",
    "same_seeds(seed)\n",
    "\n",
    "model = GAT(in_dim=50, hidden_dim=16, out_dim=168, num_heads=8)\n",
    "torch.save(model.state_dict(), 'model_initial/initial_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1806, -0.0598,  0.0091,  ...,  0.0719,  0.2496,  0.0873],\n",
       "        [ 0.1694, -0.0015, -0.0139,  ...,  0.0147,  0.0892,  0.0146],\n",
       "        [ 0.0969, -0.0595, -0.0115,  ..., -0.0474,  0.0529, -0.0565],\n",
       "        ...,\n",
       "        [-0.0433, -0.2248,  0.3002,  ...,  0.0850,  0.1621,  0.0422],\n",
       "        [ 0.2097, -0.2492,  0.0612,  ..., -0.0041,  0.0365, -0.1483],\n",
       "        [ 0.0971, -0.2221,  0.1652,  ..., -0.1312, -0.2610,  0.0077]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer1.fc.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if model really load the model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1806, -0.0598,  0.0091,  ...,  0.0719,  0.2496,  0.0873],\n",
       "        [ 0.1694, -0.0015, -0.0139,  ...,  0.0147,  0.0892,  0.0146],\n",
       "        [ 0.0969, -0.0595, -0.0115,  ..., -0.0474,  0.0529, -0.0565],\n",
       "        ...,\n",
       "        [-0.0433, -0.2248,  0.3002,  ...,  0.0850,  0.1621,  0.0422],\n",
       "        [ 0.2097, -0.2492,  0.0612,  ..., -0.0041,  0.0365, -0.1483],\n",
       "        [ 0.0971, -0.2221,  0.1652,  ..., -0.1312, -0.2610,  0.0077]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GAT(in_dim=50, hidden_dim=16, out_dim=168, num_heads=8)\n",
    "model.load_state_dict(torch.load('model_initial/initial_weight.pth'))\n",
    "model.layer1.fc.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 26 APs same as above x 5000 times and batch size = 4, model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16250/16250 [07:13<00:00, 37.50it/s]\n",
      "  0%|          | 0/18 [07:13<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count: 16250\n",
      "Epoch 0 | Train Loss: 3.9531 | Train Accuracy: 0.0382\n",
      "Validation Loss: 3.9531 | Validation Accuracy: 3.3935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'current_val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3028c6e6e6ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_val_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_val_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m#         torch.save(model.state_dict(), 'best_model.pth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'current_val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "seed = 8787\n",
    "same_seeds(seed)\n",
    "\n",
    "model = GAT(in_dim=50, hidden_dim=16, out_dim=168, num_heads=8)\n",
    "# in_dim means the dimension of the node_feat(50 dim, since the 50-dim embedding)\n",
    "# out_dim means the # of the categories -> 168 for out tasks\n",
    "# model.load_state_dict(torch.load('model_initial/initial_weight.pth'))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "total_steps = 18\n",
    "\n",
    "# save the best model\n",
    "best_val_loss = float('inf')\n",
    "patience = 3  # Number of epochs with no improvement after which training will be stopped.\n",
    "waiting = 0  # The number of epochs with no improvement so far.\n",
    "\n",
    "\n",
    "# Training Part\n",
    "for epoch in tqdm(range(total_steps)):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    count = 0 \n",
    "    \n",
    "    for data in tqdm(dataloaders['train'], position=0, leave=True):\n",
    "        \n",
    "        count += 1\n",
    "        loss, accuracy, _ = model_fn(data, model, criterion, device, count)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "#     scheduler.step()\n",
    "    print(f\"total count: {count}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "\n",
    "    print(f'Epoch {epoch} | Train Loss: {avg_loss:.4f} | Train Accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "    \n",
    "    # Validation Part\n",
    "    model.eval()\n",
    "    total_accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batched_g in dataloaders['valid']:\n",
    "            loss, accuracy, _ = model_fn(batched_g, model, criterion, device)\n",
    "            total_accuracy += accuracy.item()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    current_loss = total_loss / num_batches\n",
    "    print(f'Validation Loss: {avg_loss:.4f} | Validation Accuracy: {current_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    if current_loss < best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        waiting = 0\n",
    "#         torch.save(model.state_dict(), 'best_model.pth')\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, f\"../checkpoint_GAT/best_model_{epoch}.pt\")\n",
    "    \n",
    "    else:\n",
    "        waiting += 1\n",
    "        if waiting >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "            \n",
    "    # Save checkpoint\n",
    "#     if epoch%20 == 0:\n",
    "#         torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'loss': loss,\n",
    "#                 }, f\"../checkpoint_GAT/checkpoint_{epoch}.pt\")\n",
    "    \n",
    "\n",
    "# Testing Part\n",
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in dataloaders['test']:\n",
    "        loss, accuracy, predicted = model_fn(data, model, criterion, device)\n",
    "        labels = data[1].to(device)  # Assuming labels are the second element in the tuple\n",
    "        \n",
    "        print(f\"labels: {labels}\", labels.shape)\n",
    "        print(f\"predicted: {predicted}\", predicted.shape)\n",
    "        \n",
    "        total += labels.size(0) # label.size(0) is the batch size\n",
    "        correct += (predicted == labels).sum().item() \n",
    "        # (predicted == labels).sum() would return how many of them are equal; \n",
    "        # .item() would make the tensor to the regular value\n",
    "        \n",
    "    print('Test Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
