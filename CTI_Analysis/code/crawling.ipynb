{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the code for crawling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zoungming/.pyenv/versions/3.9.12/lib/python3.9/site-packages (from requests) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zoungming/.pyenv/versions/3.9.12/lib/python3.9/site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zoungming/.pyenv/versions/3.9.12/lib/python3.9/site-packages (from requests) (3.4)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp39-cp39-macosx_10_9_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 KB\u001b[0m \u001b[31m928.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: charset-normalizer, requests\n",
      "Successfully installed charset-normalizer-3.1.0 requests-2.31.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/zoungming/.pyenv/versions/3.9.12/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install requests\n",
    "# !pip show requests\n",
    "# !pip show beautifulsoup4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crawl the whole web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url =  \"https://securelist.com/sofacy-apt-hits-high-profile-targets-with-updated-toolset/72924/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# 遞迴函數，提取元素的文本內容\n",
    "def extract_text(element):\n",
    "    text = \"\"\n",
    "\n",
    "    # 如果元素是NavigableString對象，即表示純文本內容，直接返回其內容\n",
    "    if isinstance(element, str):\n",
    "        return element.strip()\n",
    "\n",
    "    # 遍歷子元素，遞迴提取文本內容\n",
    "    for child in element.children:\n",
    "        text += extract_text(child) + \" \"\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# 提取網頁的全部文本內容\n",
    "content = extract_text(soup)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crawl the partial of the web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://securelist.com/sofacy-apt-hits-high-profile-targets-with-updated-toolset/72924/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# 假設要提取<p>標籤內的文本內容\n",
    "# paragraphs = soup.find_all(\"p\")\n",
    "paragraphs = soup.find_all(\"div\")\n",
    "# paragraphs = soup.find(\"div\", id=\"fb-root\")\n",
    "\n",
    "content = \"\"\n",
    "for p in paragraphs:\n",
    "    content += p.text + \"\\n\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if there is existing file with the same file name and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filename = \"crawling.txt\"\n",
    "\n",
    "folder_path = \"./crawling_file\"\n",
    "\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # 获取文件的名字和扩展名\n",
    "    name, extension = os.path.splitext(filename)\n",
    "\n",
    "    # 定义新的文件名\n",
    "    new_filename = name + \"-1\" + extension\n",
    "\n",
    "    # 通过迭代查找未使用的文件名\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(folder_path, new_filename)):\n",
    "        counter += 1\n",
    "        new_filename = f\"{name}-{counter}{extension}\"\n",
    "\n",
    "    # 更新文件名\n",
    "    filename = new_filename\n",
    "\n",
    "# 将内容保存到文件\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheng-ru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7771dd1fbefba0f9e49b3f12d6cb05ea3fc9d8cb4bbb591d0ecb9d07210ade7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
