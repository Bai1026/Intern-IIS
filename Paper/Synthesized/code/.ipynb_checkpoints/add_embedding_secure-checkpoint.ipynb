{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the script to get the embedding on the synthesized dataset  \n",
    "- ``Trans Family``: directly use the word embedding -> do not consider the relation  \n",
    "- ``secureBERT``: would consider the relation and get the overall embedding -> node and relation embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The version with the PCA reduce dimension\n",
    "- dimension = [DIM, 26]\n",
    "- the original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load embedding function\n",
    "def load_embedding(input_embedding_name, model, DIM):\n",
    "    if model.startswith('trans'):\n",
    "        with open(input_embedding_name) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # trans family wouldn't consider the relation embedding -> directly use the word embedding\n",
    "        # so the dimension of the node and the edge would be the same\n",
    "        ent_embeddings = np.array(data['ent_embeddings.weight'])\n",
    "        rel_embeddings = np.array(data['rel_embeddings.weight'])\n",
    "        return ent_embeddings, rel_embeddings\n",
    "    \n",
    "    elif model == 'secureBERT':\n",
    "        ent_embeddings = np.empty((0, 768), dtype=np.float32)\n",
    "        for filename in sorted(os.listdir(input_embedding_name)):\n",
    "            print(filename)\n",
    "\n",
    "            if not filename.startswith('embeddings_chunk'):\n",
    "                continue\n",
    "\n",
    "            embedding = np.load(f'{input_embedding_name}/{filename}')\n",
    "\n",
    "            print(ent_embeddings.shape, embedding.shape)\n",
    "\n",
    "            ent_embeddings = np.concatenate((ent_embeddings, embedding), axis=0)\n",
    "            print(filename, ent_embeddings.shape)\n",
    "\n",
    "        print(f'Reducing entity embedding to ({DIM},)')\n",
    "        print(ent_embeddings.shape, '->', end=' ')\n",
    "        \n",
    "        pca = PCA(n_components=DIM)\n",
    "        ent_embeddings = pca.fit_transform(ent_embeddings)\n",
    "        print(ent_embeddings.shape)\n",
    "\n",
    "        # secureBERT would consider the edge embedding -> input is relation.npy\n",
    "        # dimension of the node -> depends on us\n",
    "        # dimension of the edge -> 26 (since PCA)\n",
    "        \n",
    "        \n",
    "        rel_embeddings = np.load(f'{input_embedding_name}/relation.npy')\n",
    "        print(f'Reducing relation embedding to ({len(rel_embeddings)},)')\n",
    "        print(rel_embeddings.shape, '->', end=' ')\n",
    "        \n",
    "        \n",
    "        pca = PCA(n_components=len(rel_embeddings))\n",
    "        rel_embeddings = pca.fit_transform(rel_embeddings)\n",
    "        print(rel_embeddings.shape)\n",
    "        return ent_embeddings, rel_embeddings\n",
    "    else:\n",
    "        print('Error!!')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The version of without PCA reduce dimension\n",
    "- node dimension = 768\n",
    "- edge dimension = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load embedding function\n",
    "def load_embedding(input_embedding_name, model):\n",
    "    if model.startswith('trans'):\n",
    "        with open(input_embedding_name) as f:\n",
    "            data = json.load(f)\n",
    "        ent_embeddings = np.array(data['ent_embeddings.weight'])\n",
    "        rel_embeddings = np.array(data['rel_embeddings.weight'])\n",
    "        return ent_embeddings, rel_embeddings\n",
    "    \n",
    "    elif model == 'secureBERT':\n",
    "        ent_embeddings = np.empty((0, 768), dtype=np.float32)\n",
    "        for filename in sorted(os.listdir(input_embedding_name)):\n",
    "            filepath = os.path.join(input_embedding_name, filename)\n",
    "            if not os.path.isfile(filepath) or not filename.startswith('embeddings_chunk'):\n",
    "                continue\n",
    "\n",
    "            embedding = np.load(filepath)\n",
    "            print(filename)\n",
    "            print(ent_embeddings.shape, embedding.shape)\n",
    "            ent_embeddings = np.concatenate((ent_embeddings, embedding), axis=0)\n",
    "            print(filename, ent_embeddings.shape)\n",
    "\n",
    "        # secureBERT would consider the edge embedding -> input is relation.npy\n",
    "        # 直接加载关系嵌入，不进行 PCA 降维\n",
    "        rel_embeddings = np.load(f'{input_embedding_name}/relation.npy')\n",
    "        return ent_embeddings, rel_embeddings\n",
    "\n",
    "    else:\n",
    "        print('Error!!')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The version of PCA with node and without PCA with edge\n",
    "- node dimension: depends on the model\n",
    "- edge dimension: 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load embedding function\n",
    "def load_embedding(input_embedding_name, model, DIM):\n",
    "    if model.startswith('trans'):\n",
    "        with open(input_embedding_name) as f:\n",
    "            data = json.load(f)\n",
    "        ent_embeddings = np.array(data['ent_embeddings.weight'])\n",
    "        rel_embeddings = np.array(data['rel_embeddings.weight'])\n",
    "        return ent_embeddings, rel_embeddings\n",
    "\n",
    "    elif model == 'secureBERT':\n",
    "        ent_embeddings = np.empty((0, 768), dtype=np.float32)\n",
    "        for filename in sorted(os.listdir(input_embedding_name)):\n",
    "            filepath = os.path.join(input_embedding_name, filename)\n",
    "            if not os.path.isfile(filepath) or not filename.startswith('embeddings_chunk'):\n",
    "                continue\n",
    "\n",
    "            embedding = np.load(filepath)\n",
    "            print(filename)\n",
    "            print(ent_embeddings.shape, embedding.shape)\n",
    "            ent_embeddings = np.concatenate((ent_embeddings, embedding), axis=0)\n",
    "            print(filename, ent_embeddings.shape)\n",
    "            \n",
    "            ent_embeddings = np.concatenate((ent_embeddings, embedding), axis=0)\n",
    "\n",
    "        # 对实体嵌入进行 PCA 降维\n",
    "        print(f'Reducing entity embedding to ({DIM},)')\n",
    "        pca = PCA(n_components=DIM)\n",
    "        ent_embeddings = pca.fit_transform(ent_embeddings)\n",
    "        print(f'Entity embeddings reduced: {ent_embeddings.shape}')\n",
    "\n",
    "        # 直接加载关系嵌入，不进行 PCA 降维\n",
    "        rel_embeddings = np.load(f'{input_embedding_name}/relation.npy')\n",
    "        print(f'Relation embeddings: {rel_embeddings.shape}')\n",
    "\n",
    "        return ent_embeddings, rel_embeddings\n",
    "\n",
    "    else:\n",
    "        print('Error!!')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have 165000 data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is for synthesized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3]\n",
    "\n",
    "print(1 in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cd4e33684548f4947d211092c53312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved in '../data/exp3/before_embedding/all_graph_modified(node_feat).jsonl'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "weird_nodes = [189923, 829358, 270488, 405143, 829356, 829357]\n",
    "input_filename = '../data/exp3/before_embedding/all_graph.jsonl'\n",
    "output_filename = '../data/exp3/before_embedding/all_graph_modified(node_feat).jsonl'\n",
    "\n",
    "with open(input_filename, 'r') as file, open(output_filename, 'w') as outfile:\n",
    "    for line in tqdm(file):\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "#         data[\"node_feat\"] = [node_id - 1 if node_id in weird_nodes else node_id for node_id in data[\"node_feat\"]]\n",
    "        data[\"node_feat\"] = [node_id - 3 if node_id in weird_nodes else node_id for node_id in data[\"node_feat\"]]\n",
    "\n",
    "        json.dump(data, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "print(f\"Modified data saved in '{output_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7167ff49fe4c4590ae9272b8e6d9a651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad8fee3afec427e9044e0bd293ed1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file name: ../data/exp3/after_embedding/secureBERT_50_embedded.jsonl\n",
      "embeddings_chunk_0.npy\n",
      "(0, 768) (160000, 768)\n",
      "embeddings_chunk_0.npy (160000, 768)\n",
      "embeddings_chunk_1.npy\n",
      "(160000, 768) (160000, 768)\n",
      "embeddings_chunk_1.npy (320000, 768)\n",
      "embeddings_chunk_2.npy\n",
      "(320000, 768) (160000, 768)\n",
      "embeddings_chunk_2.npy (480000, 768)\n",
      "embeddings_chunk_3.npy\n",
      "(480000, 768) (160000, 768)\n",
      "embeddings_chunk_3.npy (640000, 768)\n",
      "embeddings_chunk_4.npy\n",
      "(640000, 768) (160000, 768)\n",
      "embeddings_chunk_4.npy (800000, 768)\n",
      "embeddings_chunk_5.npy\n",
      "(800000, 768) (29355, 768)\n",
      "embeddings_chunk_5.npy (829355, 768)\n",
      "relation.npy\n",
      "Reducing entity embedding to (50,)\n",
      "(829355, 768) -> (829355, 50)\n",
      "Reducing relation embedding to (27,)\n",
      "(27, 768) -> (27, 27)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fc9dfbf83744f3bf680574402e6594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 829356 is out of bounds for axis 0 with size 829355",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-50aa37dc5d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"node_feat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ment_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'secureBERT'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ment_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"node_feat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"edge_attr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0medge_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"edge_attr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-50aa37dc5d4c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"node_feat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ment_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'secureBERT'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ment_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"node_feat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"edge_attr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0medge_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"edge_attr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 829356 is out of bounds for axis 0 with size 829355"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embedding_files = [\"../data/4_embedding/synthesize/secureBERT\"]\n",
    "model = 'secureBERT'\n",
    "\n",
    "# 输入文件列表\n",
    "# input_filenames = [\"../data_new/graph/benign/graph_benign.jsonl\"]\n",
    "# input_filenames = [\"../data_new/graph/without_benign/graph_without_benign.jsonl\"]\n",
    "input_filenames = [\"../data/exp3/before_embedding/all_graph_modified(node_feat).jsonl\"]\n",
    "# input_filenames = [\"../data/exp3/before_embedding/test.jsonl\"]\n",
    "\n",
    "# for i in tqdm(range(3)):\n",
    "#     DIM = 150 - 50*i\n",
    "DIM = 50\n",
    "\n",
    "for input_filename in tqdm(input_filenames):\n",
    "    print(\"Start!\")\n",
    "    base, ext = os.path.splitext(input_filename)\n",
    "\n",
    "    with open(input_filename, \"r\") as f:\n",
    "        input_data = list(f)\n",
    "\n",
    "    for embedding_file in tqdm(embedding_files):\n",
    "        # output_filename = f\"../data_new/exp3/graph/secureBERT_{DIM}_embedded.jsonl\"\n",
    "        output_filename = f\"../data/exp3/after_embedding/secureBERT_{DIM}_embedded.jsonl\"\n",
    "#         output_filename = f\"../data/exp3/secureBERT_{DIM}_embedded_test.jsonl\"\n",
    "\n",
    "        print(f\"output file name: {output_filename}\")\n",
    "\n",
    "        with open(output_filename, \"w\") as out_file:\n",
    "            model = embedding_file.split('/')[-1].split('_')[0]\n",
    "            # ent_embeddings, rel_embeddings = load_embedding(embedding_file, model)\n",
    "            ent_embeddings, rel_embeddings = load_embedding(embedding_file, model, DIM)\n",
    "\n",
    "\n",
    "            for line, data in tqdm(zip(input_data, input_data)):\n",
    "                data = json.loads(data.strip())\n",
    "                \n",
    "                data[\"node_feat\"] = [ent_embeddings[node_id].tolist() if model == 'secureBERT' else ent_embeddings[node_id] for node_id in data[\"node_feat\"]]\n",
    "\n",
    "                data[\"edge_attr\"] = [rel_embeddings[edge_id].tolist() for edge_id in data[\"edge_attr\"]]\n",
    "\n",
    "                # Convert the data back to a JSON string and write to the output file\n",
    "                out_file.write(json.dumps(data) + '\\n')\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "# ==========for handling the weird nodes (4 of them)========== 2nd modified\n",
    "#                 new_node_feats = []\n",
    "#                 weird_nodes = [189923, 829358, 270488, 405143]\n",
    "        \n",
    "#                 data[\"node_feat\"] = []\n",
    "#                 for node_id in data[\"node_feat\"]:\n",
    "#                     if node_id in weird_nodes and node_id > 0:\n",
    "#                         embedding = ent_embeddings[node_id - 1]\n",
    "#                     else:\n",
    "#                         embedding = ent_embeddings[node_id]\n",
    "\n",
    "#                     if model == 'secureBERT':\n",
    "#                         embedding = embedding.tolist()\n",
    "\n",
    "#                     data[\"node_feat\"].append(embedding)\n",
    "                \n",
    "                \n",
    "\n",
    "# ==========for handling the weird nodes (4 of them)========== 1st modified\n",
    "# ==========if index error -> use the last one -> need to be fixed==========\n",
    "#             for line, data in tqdm(zip(input_data, input_data)):\n",
    "#                 data = json.loads(data.strip())\n",
    "\n",
    "#                 # 处理节点特征\n",
    "#                 data[\"node_feat\"] = []\n",
    "#                 for node_id in data[\"node_feat\"]:\n",
    "#                     if node_id < len(ent_embeddings):\n",
    "#                         embedding = ent_embeddings[node_id]\n",
    "#                     else:\n",
    "#                         # 如果索引不存在，使用最接近的有效索引\n",
    "#                         nearest_valid_index = min(node_id, len(ent_embeddings) - 1)\n",
    "#                         embedding = ent_embeddings[nearest_valid_index]\n",
    "\n",
    "#                     data[\"node_feat\"].append(embedding.tolist() if model == 'secureBERT' else embedding)\n",
    "\n",
    "\n",
    "\n",
    "#                 Replace node_feat and edge_attr with embeddings =====> original version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is for DARPA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b97f70f921047878b87e3879a227d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file name: /workdir/home/bai/Euni_HO_modified/data/training_data/secureBERT_150_embedded(edge768).jsonl\n",
      "embeddings_chunk_0.npy\n",
      "(0, 768) (160000, 768)\n",
      "embeddings_chunk_0.npy (160000, 768)\n",
      "embeddings_chunk_1.npy\n",
      "(320000, 768) (160000, 768)\n",
      "embeddings_chunk_1.npy (480000, 768)\n",
      "embeddings_chunk_2.npy\n",
      "(640000, 768) (160000, 768)\n",
      "embeddings_chunk_2.npy (800000, 768)\n",
      "embeddings_chunk_3.npy\n",
      "(960000, 768) (20281, 768)\n",
      "embeddings_chunk_3.npy (980281, 768)\n",
      "Reducing entity embedding to (150,)\n",
      "Entity embeddings reduced: (1000562, 150)\n",
      "Relation embeddings: (23, 768)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8563ad2253343b28bc311ceb48aa108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embedding_files = [\"/workdir/home/bai/Euni_HO_modified/data/4_embedding/synthesize/secureBERT\"]\n",
    "model = 'secureBERT'\n",
    "\n",
    "# 输入文件列表\n",
    "input_filenames = '/workdir/home/bai/Euni_HO_modified/data/3_openKE/synthesize/all_graph_data.jsonl'\n",
    "\n",
    "# for i in tqdm(range(3)):\n",
    "#     DIM = 150 - 50*i\n",
    "DIM = 150\n",
    "\n",
    "print(\"Start!\")\n",
    "# base, ext = os.path.splitext(input_filenames)\n",
    "\n",
    "with open(input_filenames, \"r\") as f:\n",
    "    input_data = list(f)\n",
    "\n",
    "for embedding_file in tqdm(embedding_files):\n",
    "    # output_filename = f\"../data_new/exp3/graph/secureBERT_{DIM}_embedded.jsonl\"\n",
    "#         output_filename = f\"../data_new/exp3/graph/secureBERT_{DIM}_embedded(edge768).jsonl\"\n",
    "    output_filename = f\"/workdir/home/bai/Euni_HO_modified/data/training_data/secureBERT_{DIM}_embedded(edge768).jsonl\"\n",
    "\n",
    "    print(f\"output file name: {output_filename}\")\n",
    "\n",
    "    with open(output_filename, \"w\") as out_file:\n",
    "        model = embedding_file.split('/')[-1].split('_')[0]\n",
    "        # ent_embeddings, rel_embeddings = load_embedding(embedding_file, model)\n",
    "        ent_embeddings, rel_embeddings = load_embedding(embedding_file, model, DIM)\n",
    "        # ...\n",
    "\n",
    "        for line, data in tqdm(zip(input_data, input_data)):\n",
    "            data = json.loads(data.strip())\n",
    "\n",
    "            # Replace node_feat and edge_attr with embeddings\n",
    "            data[\"node_feat\"] = [ent_embeddings[node_id].tolist() if model == 'secureBERT' else ent_embeddings[node_id] for node_id in data[\"node_feat\"]]\n",
    "            data[\"edge_attr\"] = [rel_embeddings[edge_id].tolist() for edge_id in data[\"edge_attr\"]]\n",
    "\n",
    "            # Convert the data back to a JSON string and write to the output file\n",
    "            out_file.write(json.dumps(data) + '\\n')\n",
    "            \n",
    "# for input_filename in tqdm(input_filenames):\n",
    "#     print(\"Start!\")\n",
    "#     base, ext = os.path.splitext(input_filename)\n",
    "\n",
    "#     with open(input_filename, \"r\") as f:\n",
    "#         input_data = list(f)\n",
    "\n",
    "#     for embedding_file in tqdm(embedding_files):\n",
    "#         # output_filename = f\"../data_new/exp3/graph/secureBERT_{DIM}_embedded.jsonl\"\n",
    "# #         output_filename = f\"../data_new/exp3/graph/secureBERT_{DIM}_embedded(edge768).jsonl\"\n",
    "#         output_filename = f\"/workdir/home/bai/Euni_HO_modified/data/training_data/secureBERT_{DIM}_embedded(edge768).jsonl\"\n",
    "\n",
    "#         print(f\"output file name: {output_filename}\")\n",
    "\n",
    "#         with open(output_filename, \"w\") as out_file:\n",
    "#             model = embedding_file.split('/')[-1].split('_')[0]\n",
    "#             # ent_embeddings, rel_embeddings = load_embedding(embedding_file, model)\n",
    "#             ent_embeddings, rel_embeddings = load_embedding(embedding_file, model, DIM)\n",
    "#             # ...\n",
    "\n",
    "#             for line, data in tqdm(zip(input_data, input_data)):\n",
    "#                 data = json.loads(data.strip())\n",
    "\n",
    "#                 # Replace node_feat and edge_attr with embeddings\n",
    "#                 data[\"node_feat\"] = [ent_embeddings[node_id].tolist() if model == 'secureBERT' else ent_embeddings[node_id] for node_id in data[\"node_feat\"]]\n",
    "#                 data[\"edge_attr\"] = [rel_embeddings[edge_id].tolist() for edge_id in data[\"edge_attr\"]]\n",
    "\n",
    "#                 # Convert the data back to a JSON string and write to the output file\n",
    "#                 out_file.write(json.dumps(data) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine 2 jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2657470da67416d820e16162bd90d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_new/graph/with_benign/secureBERT_150_embedded.jsonl\n",
      "../data_new/graph/with_benign/secureBERT_100_embedded.jsonl\n",
      "../data_new/graph/with_benign/secureBERT_50_embedded.jsonl\n"
     ]
    }
   ],
   "source": [
    "# files = ['transE_50', 'transE_100', 'transE_150', 'transH_50', 'transH_100', 'transH_150', 'transR_50',\n",
    "#          'secureBERT_250', 'secureBERT_150', 'secureBERT_100', 'secureBERT_50']\n",
    "files = ['secureBERT_150', 'secureBERT_100', 'secureBERT_50']\n",
    "\n",
    "for file in tqdm(files):\n",
    "    file1 = f\"../data_new/graph/benign/{file}_embedded.jsonl\"\n",
    "    data1 = []\n",
    "\n",
    "    with open(file1, 'r') as f:\n",
    "        for line in f:\n",
    "            data1.append(json.loads(line))\n",
    "\n",
    "    file2 = f\"../data_new/graph/without_benign/{file}_embedded.jsonl\"\n",
    "    data2 = []\n",
    "\n",
    "    with open(file2, 'r') as f:\n",
    "        for line in f:\n",
    "            data2.append(json.loads(line))\n",
    "\n",
    "    combined_data = data1 + data2\n",
    "\n",
    "    output_file = f\"../data_new/graph/with_benign/{file}_embedded.jsonl\"\n",
    "    print(output_file)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in combined_data:\n",
    "            f.write(json.dumps(item) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
