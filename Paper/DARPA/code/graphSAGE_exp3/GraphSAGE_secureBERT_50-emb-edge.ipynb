{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of GraphSAGE\n",
    "- use DGL\n",
    "- predict `graphs`\n",
    "- valid, test data are in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dgl\n",
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import subprocess\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from dgl.nn import GraphConv, GATConv, SAGEConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the GPU and assign the GPU by the best memory usage\n",
    "- use cuda:0 in A100 server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# def get_free_gpu():\n",
    "#     try:\n",
    "#         # Run nvidia-smi command to get GPU details\n",
    "#         _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "#         command = \"nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader\"\n",
    "#         memory_free_info = _output_to_list(subprocess.check_output(command.split())) \n",
    "#         memory_free_values = [int(x) for i, x in enumerate(memory_free_info)]\n",
    "        \n",
    "#         # Get the GPU with the maximum free memory\n",
    "#         best_gpu_id = memory_free_values.index(max(memory_free_values))\n",
    "#         return best_gpu_id\n",
    "#     except:\n",
    "#         # If any exception occurs, default to GPU 0 (this handles cases where nvidia-smi isn't installed)\n",
    "#         return 0\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     # Get the best GPU ID based on free memory and set it\n",
    "#     best_gpu_id = get_free_gpu()\n",
    "#     device = torch.device(f\"cuda:{best_gpu_id}\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"there's no available GPU\")\n",
    "\n",
    "device = torch.device(f\"cuda:{0}\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix seed\n",
    "def same_seeds(seed = 8787):\n",
    "    torch.manual_seed(seed)\n",
    "    # random.seed(seed) \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding function\n",
    "def load_embedding(input_embedding_name, model):\n",
    "    if model.startswith('trans'):\n",
    "        with open(input_embedding_name) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # trans family wouldn't consider the relation embedding -> directly use the word embedding\n",
    "        # so the dimension of the node and the edge would be the same\n",
    "        ent_embeddings = np.array(data['ent_embeddings.weight'])\n",
    "        rel_embeddings = np.array(data['rel_embeddings.weight'])\n",
    "        return ent_embeddings, rel_embeddings\n",
    "    \n",
    "    elif model == 'secureBERT':\n",
    "        ent_embeddings = np.empty((0, 768), dtype=np.float32)\n",
    "        for filename in sorted(os.listdir(input_embedding_name)):\n",
    "            print(filename)\n",
    "\n",
    "            if not filename.startswith('embeddings_chunk'):\n",
    "                continue\n",
    "\n",
    "            embedding = np.load(f'{input_embedding_name}/{filename}')\n",
    "\n",
    "            print(ent_embeddings.shape, embedding.shape)\n",
    "\n",
    "            ent_embeddings = np.concatenate((ent_embeddings, embedding), axis=0)\n",
    "            print(filename, ent_embeddings.shape)\n",
    "\n",
    "        print(f'Reducing entity embedding to ({DIM},)')\n",
    "        print(ent_embeddings.shape, '->', end=' ')\n",
    "        \n",
    "        pca = PCA(n_components=DIM)\n",
    "        ent_embeddings = pca.fit_transform(ent_embeddings)\n",
    "        print(ent_embeddings.shape)\n",
    "\n",
    "        # secureBERT would consider the edge embedding -> input is relation.npy\n",
    "        # dimension of the node -> depends on us\n",
    "        # dimension of the edge -> edge_number (since PCA)\n",
    "        rel_embeddings = np.load(f'{input_embedding_name}/relation.npy')\n",
    "        print(f'Reducing relation embedding to ({len(rel_embeddings)},)')\n",
    "        print(rel_embeddings.shape, '->', end=' ')\n",
    "        pca = PCA(n_components=len(rel_embeddings))\n",
    "        rel_embeddings = pca.fit_transform(rel_embeddings)\n",
    "        print(rel_embeddings.shape)\n",
    "        return ent_embeddings, rel_embeddings\n",
    "    else:\n",
    "        print('Error!!')\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Load embedding function\n",
    "# def load_embedding(input_embedding_name, model):\n",
    "#     if model.startswith('trans'):\n",
    "#         with open(input_embedding_name) as f:\n",
    "#             data = json.load(f)\n",
    "#         ent_embeddings = np.array(data['ent_embeddings.weight'])\n",
    "#         rel_embeddings = np.array(data['rel_embeddings.weight'])\n",
    "#         return ent_embeddings, rel_embeddings\n",
    "\n",
    "#     elif model == 'secureBERT':\n",
    "#         ent_embeddings = np.empty((0, 768), dtype=np.float32)\n",
    "#         for filename in sorted(os.listdir(input_embedding_name)):\n",
    "#             filepath = os.path.join(input_embedding_name, filename)\n",
    "#             if not os.path.isfile(filepath) or not filename.startswith('embeddings_chunk'):\n",
    "#                 continue\n",
    "\n",
    "#             embedding = np.load(filepath)\n",
    "#             print(filename)\n",
    "#             print(ent_embeddings.shape, embedding.shape)\n",
    "#             ent_embeddings = np.concatenate((ent_embeddings, embedding), axis=0)\n",
    "#             print(filename, ent_embeddings.shape)\n",
    "            \n",
    "#             ent_embeddings = np.concatenate((ent_embeddings, embedding), axis=0)\n",
    "\n",
    "#         # 对实体嵌入进行 PCA 降维\n",
    "#         print(f'Reducing entity embedding to ({DIM},)')\n",
    "#         pca_ent = PCA(n_components=DIM)\n",
    "#         ent_embeddings = pca_ent.fit_transform(ent_embeddings)\n",
    "#         print(f'Entity embeddings reduced: {ent_embeddings.shape}')\n",
    "        \n",
    "#         # 直接加载关系嵌入，不进行 PCA 降维\n",
    "#         rel_embeddings = np.load(f'{input_embedding_name}/relation.npy')\n",
    "#         print(f'Relation embeddings: {rel_embeddings.shape}')\n",
    "\n",
    "#         return ent_embeddings, rel_embeddings\n",
    "\n",
    "#     else:\n",
    "#         print('Error!!')\n",
    "#         return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"../../data/4_embedding/synthesize/secureBERT\"\n",
    "input_filename = '../../data/source_data/before_embedding/3.10/all_graph_data.jsonl'\n",
    "\n",
    "model = 'secureBERT'\n",
    "DIM = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b7fd33d18d40539832d6d678a22809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading:   0%|          | 0/399000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH...\n",
      "399000\n"
     ]
    }
   ],
   "source": [
    "with open(input_filename, \"r\") as f:\n",
    "    print(\"Loading the data...\")\n",
    "\n",
    "    # only process 40000 data from 400000 data\n",
    "    wanted_data = 399000\n",
    "    input_data = []\n",
    "    for idx, line in tqdm(enumerate(f), total=wanted_data, desc=\"Loading\"):\n",
    "        if idx == wanted_data:\n",
    "            break\n",
    "        input_data.append(json.loads(line))\n",
    "        \n",
    "    print(\"FINISH...\")\n",
    "\n",
    "print(len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Embedding ...\n",
      "embeddings_chunk_0.npy\n",
      "(0, 768) (160000, 768)\n",
      "embeddings_chunk_0.npy (160000, 768)\n",
      "embeddings_chunk_1.npy\n",
      "(160000, 768) (160000, 768)\n",
      "embeddings_chunk_1.npy (320000, 768)\n",
      "embeddings_chunk_2.npy\n",
      "(320000, 768) (160000, 768)\n",
      "embeddings_chunk_2.npy (480000, 768)\n",
      "embeddings_chunk_3.npy\n",
      "(480000, 768) (20281, 768)\n",
      "embeddings_chunk_3.npy (500281, 768)\n",
      "relation.npy\n",
      "Reducing entity embedding to (150,)\n",
      "(500281, 768) -> (500281, 150)\n",
      "Reducing relation embedding to (23,)\n",
      "(23, 768) -> (23, 23)\n",
      "Process Embedding ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048628f1926e4bdeb8eaacc22a5d7980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Embedding:   0%|          | 0/399000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = embedding_file.split('/')[-1].split('_')[0]\n",
    "print('Load Embedding ...')\n",
    "ent_embeddings, rel_embeddings = load_embedding(embedding_file, model)\n",
    "\n",
    "print('Process Embedding ...')\n",
    "# if not tolist(), the original format is array -> error format\n",
    "for data_point in tqdm(input_data, desc='Processing Embedding'):\n",
    "    data_point['node_feat'] = [ent_embeddings[node_id].tolist() for node_id in data_point['node_feat']]\n",
    "    data_point['edge_attr'] = [rel_embeddings[edge_id].tolist() for edge_id in data_point['edge_attr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_data[0]['node_feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_data[0]['edge_attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_list, device):\n",
    "        self.data_list = data_list\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "        return data\n",
    "\n",
    "def collate(samples):\n",
    "    data_list = samples\n",
    "    batched_graphs = []\n",
    "    for data in data_list:\n",
    "        g = dgl.graph((th.tensor(data[\"edge_index\"][0]), th.tensor(data[\"edge_index\"][1])), num_nodes=data[\"num_nodes\"])\n",
    "\n",
    "        g.ndata['feat'] = th.tensor(data[\"node_feat\"])\n",
    "        g.edata['feat'] = th.tensor(data[\"edge_attr\"])\n",
    "        g.edata['label'] = th.tensor(data[\"labels\"])  # Add edge labels to graph\n",
    "\n",
    "        batched_graphs.append(g)\n",
    "    \n",
    "    return dgl.batch(batched_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = len(input_data)\n",
    "\n",
    "test_size = int(total_data * 0.1)\n",
    "train_valid_size = total_data - test_size\n",
    "\n",
    "train_valid_data = input_data[:train_valid_size]\n",
    "test_data = input_data[train_valid_size:]\n",
    "\n",
    "train_data, valid_data = train_test_split(train_valid_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# creating GraphDataset\n",
    "dataset_data = {\n",
    "    'train': GraphDataset(train_data, device),\n",
    "    'valid': GraphDataset(valid_data, device),\n",
    "    'test': GraphDataset(test_data, device)\n",
    "}\n",
    "\n",
    "print(\"Datasets loaded and ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- choose batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(batch_size, shuffle=True):\n",
    "    dataloaders = {}\n",
    "    for dataset_name, dataset in dataset_data.items():\n",
    "        # do not shuffle the testing dataset\n",
    "        if dataset_name == \"test\":\n",
    "            dataloaders[dataset_name] = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)    \n",
    "        else:\n",
    "            dataloaders[dataset_name] = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate)\n",
    "    return dataloaders\n",
    "\n",
    "dataloaders = create_dataloaders(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_embedding_dim = 0\n",
    "\n",
    "# Assuming dataloaders is a dictionary with 'test' as one of the keys\n",
    "for batch in dataloaders['train']:\n",
    "    # Your batch processing code here\n",
    "    print(batch, \"\\n\")\n",
    "#     print(\"edata:\", batch.edata, '\\n')\n",
    "    print(\"edata['feat'] size:\", batch.edata['feat'].shape, '\\n')\n",
    "    print(\"edata['label']:\", batch.edata['label'])\n",
    "\n",
    "    edge_embedding_dim = batch.edata['feat'].shape[1]\n",
    "\n",
    "    break  # To break out after the first batch if needed\n",
    "\n",
    "print(\"\\n\\nedge embedding dimension: \", edge_embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Turn the print message to a log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "formatted_time = now.strftime(\"%m%d_%H:%M\")\n",
    "\n",
    "log_file_path = f\"./log_message/{formatted_time}_GraphSAGE_secureBERT_50-plusedge.log\"\n",
    "\n",
    "def add_log_msg(msg, log_file_path=log_file_path):\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        f.write(f'{datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}# {msg}\\n')\n",
    "    print(f'{datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}# {msg}')\n",
    "\n",
    "print(log_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layer1 = dglnn.SAGEConv(in_dim, hidden_dim, 'pool')\n",
    "        self.layer2 = dglnn.SAGEConv(hidden_dim, out_dim, 'pool')\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.layer1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "#         h = self.dropout(h)\n",
    "        h = self.layer2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, out_feats, out_classes, edge_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(out_feats*2 + edge_embedding_dim, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges, edge_feat):\n",
    "#     def apply_edges(self, edges):\n",
    "\n",
    "        h_u = edges.src['new_node_feat']\n",
    "        h_v = edges.dst['new_node_feat']\n",
    "        \n",
    "        num_edges, edge_feat_dim = edge_feat.shape\n",
    "#         print(num_edges, edge_feat_dim)\n",
    "        \n",
    "        h_e = edge_feat\n",
    "        \n",
    "        # concat 3 features\n",
    "#         test = torch.cat([h_u, h_v, h_e],1)\n",
    "#         print(\"with edge: \", test.shape)\n",
    "        \n",
    "#         test = torch.cat([h_u, h_v],1)\n",
    "#         print(\"without edge: \", test.shape)\n",
    "        \n",
    "        score = self.W(torch.cat([h_u, h_v, h_e], 1))\n",
    "#         score = self.W(torch.cat([h_u, h_v], 1))\n",
    "\n",
    "        return {'score': score}\n",
    "\n",
    "\n",
    "    def forward(self, graph, new_node_feat, edge_feat):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['new_node_feat'] = new_node_feat\n",
    "#             graph.apply_edges(self.apply_edges)\n",
    "\n",
    "            # 在 apply_edges 时传递 edge_feat\n",
    "            graph.apply_edges(lambda edges: self.apply_edges(edges, edge_feat))\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_classes, edge_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.sage = GraphSAGE(in_features, hidden_features, out_features)\n",
    "        self.pred = MLPPredictor(out_features, num_classes, edge_embedding_dim)\n",
    "      \n",
    "    def forward(self, g, node_feat, edge_feat, return_logits=False):\n",
    "        new_node_feat = self.sage(g, node_feat)\n",
    "        logits = self.pred(g, new_node_feat, edge_feat)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Forward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(batched_g, model, criterion, device, count=1, which_type='train'):\n",
    "    \"\"\"Forward a batch through the model.\"\"\"\n",
    "#     batched_g, labels = data\n",
    "    batched_g = batched_g.to(device)\n",
    "    \n",
    "    labels = batched_g.edata['label'].to(device)\n",
    "    \n",
    "#     logits = model(batched_g, batched_g.ndata['feat'].float())\n",
    "    logits = model(batched_g, batched_g.ndata['feat'].float(), batched_g.edata['feat'].float())\n",
    "\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    output = torch.softmax(logits, dim=1)\n",
    "    preds = output.argmax(1)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = torch.mean((preds == labels).float())\n",
    "        \n",
    "    return loss, accuracy, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define all the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8787\n",
    "in_dim = DIM # dimension of the node feature\n",
    "hidden_dim = 64\n",
    "out_dim = 128\n",
    "num_classes = 2 # for DARPA\n",
    "edge_dim = edge_embedding_dim\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "total_steps = 100\n",
    "patience = 5\n",
    "waiting = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(in_dim, hidden_dim, out_dim, num_classes, edge_dim)\n",
    "best_model_path = \"./checkpoint_graphSAGE/best_model_GraphSAGE_secureBERT_50-plusedge.pt\"\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr)\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=36, eta_min=0, last_epoch=- 1, verbose=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "same_seeds(seed)\n",
    "model = model.to(device)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "# Training Part\n",
    "for epoch in tqdm(range(total_steps)):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batched_g in tqdm(dataloaders['train'], desc=\"Training\", position=0, leave=True):\n",
    "        num_batches += 1\n",
    "        loss, accuracy, _ = model_fn(batched_g, model, criterion, device, num_batches, which_type='train')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    add_log_msg(f\"total batches: {num_batches}\")\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "\n",
    "    add_log_msg(f'Epoch {epoch} | Train Loss: {avg_loss:.4f} | Train Accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "    \n",
    "    # Validation Part\n",
    "    model.eval()\n",
    "    total_accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batched_g in tqdm(dataloaders['valid'], desc=\"Validation\", position=0, leave=True):\n",
    "            loss, accuracy, _ = model_fn(batched_g, model, criterion, device, num_batches, which_type='validation')\n",
    "            total_accuracy += accuracy.item()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    current_loss = total_loss / num_batches\n",
    "    \n",
    "    add_log_msg(f'Validation Loss: {current_loss:.4f} | Validation Accuracy: {avg_accuracy:.4f}\\n')\n",
    "    \n",
    "            \n",
    "    if current_loss < best_val_loss:\n",
    "        best_val_loss = current_loss\n",
    "        waiting = 0\n",
    "        \n",
    "        if os.path.exists(best_model_path):\n",
    "            os.remove(best_model_path)\n",
    "            add_log_msg(\"Find a better model!!\")\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    " \n",
    "    else:\n",
    "        waiting += 1\n",
    "        if waiting >= patience:\n",
    "            add_log_msg(\"============================== Early stopping ==================================\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 60 APs in training x 10000times\n",
    "- 5 APs in validation x 4 times\n",
    "- 3 APs in test x 4 times\n",
    "- Batch size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the pretrained model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batched_g in tqdm(dataloaders['test'], desc=\"Testing\", position=0, leave=True):\n",
    "\n",
    "        loss, accuracy, predicted = model_fn(batched_g, model, criterion, device, count, which_type='test')\n",
    "        labels = batched_g.edata['label'].to(device)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "add_log_msg(f'Test Accuracy: {100 * correct / total} %\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fix the seed and save the model.state_dict that contains the initial weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 8787\n",
    "# same_seeds(seed)\n",
    "\n",
    "# model = Model(in_features=50, hidden_features=64, out_features=128, num_classes=167)\n",
    "# torch.save(model.state_dict(), 'model3_initial(graphsage)/initial_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.layer1.fc_self.weight\n",
    "# model.sage.layer1.fc_self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if model really load the model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(in_features=50, hidden_features=64, out_features=128, num_classes=167)\n",
    "# model.load_state_dict(torch.load('model3_initial(graphsage)/initial_weight.pth'))\n",
    "# model.sage.layer1.fc_self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model3_initial(graphsage)/initial_weight.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### If wanna output the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_data = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "# report_df = pd.DataFrame(report_data).transpose()\n",
    "\n",
    "# report_folder = 'classification_report'\n",
    "# os.makedirs(report_folder, exist_ok=True)\n",
    "\n",
    "# count = 0\n",
    "# while True:\n",
    "#     report_filename = f'classification_report-transE_50-graphSAGE-{count}.xlsx'\n",
    "#     labels_filename = f'mapped_true_predicted_labels-transE_50-graphSAGE-{count}.xlsx'\n",
    "    \n",
    "#     report_path = os.path.join(report_folder, report_filename)\n",
    "#     labels_path = os.path.join(report_folder, labels_filename)\n",
    "    \n",
    "#     if not os.path.exists(report_path) and not os.path.exists(labels_path):\n",
    "#         break\n",
    "#     count += 1\n",
    "\n",
    "    \n",
    "# report_df.to_excel(report_path, index_label='Label')\n",
    "\n",
    "# labels_df = pd.DataFrame({'true_label': true_labels, 'predicted_label': predicted_labels})\n",
    "# labels_df.to_excel(labels_path, index=False)\n",
    "\n",
    "# add_log_msg(f\"report path: {report_path}\")\n",
    "# add_log_msg(f\"label path: {labels_path}\")\n",
    "\n",
    "# report = classification_report(true_labels, predicted_labels)\n",
    "# add_log_msg(f\"report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
