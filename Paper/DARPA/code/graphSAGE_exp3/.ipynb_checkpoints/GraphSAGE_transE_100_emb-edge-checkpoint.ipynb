{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of GraphSAGE\n",
    "- use DGL\n",
    "- predict `graphs`\n",
    "- valid, test data are in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dgl\n",
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from dgl.nn import GraphConv, GATConv, SAGEConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the GPU and assign the GPU by the best memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_free_gpu():\n",
    "    try:\n",
    "        # Run nvidia-smi command to get GPU details\n",
    "        _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "        command = \"nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader\"\n",
    "        memory_free_info = _output_to_list(subprocess.check_output(command.split())) \n",
    "        memory_free_values = [int(x) for i, x in enumerate(memory_free_info)]\n",
    "        \n",
    "        # Get the GPU with the maximum free memory\n",
    "        best_gpu_id = memory_free_values.index(max(memory_free_values))\n",
    "        return best_gpu_id\n",
    "    except:\n",
    "        # If any exception occurs, default to GPU 0 (this handles cases where nvidia-smi isn't installed)\n",
    "        return 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Get the best GPU ID based on free memory and set it\n",
    "    best_gpu_id = get_free_gpu()\n",
    "    device = torch.device(f\"cuda:{best_gpu_id}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"there's no available GPU\")\n",
    "\n",
    "# device = torch.device(f\"cuda:{1}\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix seed\n",
    "def same_seeds(seed = 8787):\n",
    "    torch.manual_seed(seed)\n",
    "    # random.seed(seed) \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = 'transE_100'\n",
    "with open(f\"../../data/4_embedding/synthesize/{embedding}.vec.json\", \"r\") as f:\n",
    "    tmp = json.load(f)\n",
    "\n",
    "index2entemb = {idx:emb for idx, emb in enumerate(tmp[\"ent_embeddings.weight\"])}\n",
    "index2relemb = {idx:emb for idx, emb in enumerate(tmp[\"rel_embeddings.weight\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26868"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index2entemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index2relemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7d59a150fd4b5a9dc38a984977009e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading:   0%|          | 0/399000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH...\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/source_data/before_embedding/3.10/all_graph_data.jsonl\", \"r\") as f:\n",
    "    print(\"Loading the data...\")\n",
    "\n",
    "    # only process 40000 data from 400000 data\n",
    "    wanted_data = 399000\n",
    "    input_data = []\n",
    "    for idx, line in tqdm(enumerate(f), total=wanted_data, desc=\"Loading\"):\n",
    "        if idx == wanted_data:\n",
    "            break\n",
    "        input_data.append(json.loads(line))\n",
    "        \n",
    "    print(\"FINISH...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the 'node_feat' and 'edge_attr' from int to embedding\n",
    "    - takes about 45 min to transform the embedding\n",
    "    - if use original method -> takes about 60 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d95ab88e69b4ed98cf6f893924b3d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/399000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If type(input_data[0] == dict)\n",
    "for data_point in tqdm(input_data):\n",
    "    data_point['node_feat'] = [index2entemb[node_id] for node_id in data_point['node_feat']]\n",
    "    data_point['edge_attr'] = [index2relemb[edge_id] for edge_id in data_point['edge_attr']]\n",
    "\n",
    "\n",
    "# If type(input_data[0] == str)\n",
    "# for idx, data in tqdm(enumerate(input_data)):\n",
    "    \n",
    "#     # make the data from string to int\n",
    "#     data_point = json.loads(data)\n",
    "\n",
    "#     data_point['node_feat'] = [index2entemb[node_id] for node_id in data_point['node_feat']]\n",
    "#     data_point['edge_attr'] = [index2relemb[edge_id] for edge_id in data_point['edge_attr']]\n",
    "\n",
    "#     input_data[idx] = data_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_list, device):\n",
    "        self.data_list = data_list\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "        return data\n",
    "\n",
    "def collate(samples):\n",
    "    data_list = samples\n",
    "    batched_graphs = []\n",
    "    for data in data_list:\n",
    "        g = dgl.graph((th.tensor(data[\"edge_index\"][0]), th.tensor(data[\"edge_index\"][1])), num_nodes=data[\"num_nodes\"])\n",
    "\n",
    "        g.ndata['feat'] = th.tensor(data[\"node_feat\"])\n",
    "        g.edata['feat'] = th.tensor(data[\"edge_attr\"])\n",
    "        g.edata['label'] = th.tensor(data[\"labels\"])  # Add edge labels to graph\n",
    "\n",
    "        batched_graphs.append(g)\n",
    "    \n",
    "    return dgl.batch(batched_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 8:1:1 (train, valid, test)\n",
    "train_data, test_data = train_test_split(input_data, test_size=0.2, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "dataset_data = {\n",
    "    'train': GraphDataset(train_data, device),\n",
    "    'valid': GraphDataset(valid_data, device),\n",
    "    'test': GraphDataset(test_data, device)\n",
    "}\n",
    "\n",
    "print(\"Datasets loaded and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- choose batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(batch_size, shuffle=True):\n",
    "    dataloaders = {}\n",
    "    for dataset_name, dataset in dataset_data.items():\n",
    "        # do not shuffle the testing dataset\n",
    "        if dataset_name == \"test\":\n",
    "            dataloaders[dataset_name] = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)    \n",
    "        else:\n",
    "            dataloaders[dataset_name] = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate)\n",
    "    return dataloaders\n",
    "\n",
    "dataloaders = create_dataloaders(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_embedding_dim = 0\n",
    "\n",
    "# Assuming dataloaders is a dictionary with 'test' as one of the keys\n",
    "for batch in dataloaders['train']:\n",
    "    print(batch, \"\\n\")\n",
    "    \n",
    "#     print(\"edata:\", batch.edata, '\\n')\n",
    "    print(\"edata['feat'] size:\", batch.edata['feat'].shape, '\\n')\n",
    "    print(\"edata['label']:\", batch.edata['label'])\n",
    "\n",
    "    edge_embedding_dim = batch.edata['feat'].shape[1]\n",
    "\n",
    "    break  # To break out after the first batch if needed\n",
    "\n",
    "print(\"\\n\\nedge embedding dimension: \", edge_embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Turn the print message to a log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "formatted_time = now.strftime(\"%m%d_%H:%M\")\n",
    "\n",
    "log_file_path = f\"./log_message/{formatted_time}_GraphSAGE_transE_100-edge.log\"\n",
    "\n",
    "def add_log_msg(msg, log_file_path=log_file_path):\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        f.write(f'{datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}# {msg}\\n')\n",
    "    print(f'{datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}# {msg}')\n",
    "\n",
    "print(log_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layer1 = dglnn.SAGEConv(in_dim, hidden_dim, 'pool')\n",
    "        self.layer2 = dglnn.SAGEConv(hidden_dim, out_dim, 'pool')\n",
    "#         self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.layer1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "#         h = self.dropout(h)\n",
    "        new_node_feat = self.layer2(g, h)\n",
    "    \n",
    "        return new_node_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, out_feats, out_classes, edge_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(out_feats*2 + edge_embedding_dim, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges, edge_feat):\n",
    "#     def apply_edges(self, edges):\n",
    "\n",
    "        h_u = edges.src['new_node_feat']\n",
    "        h_v = edges.dst['new_node_feat']\n",
    "        \n",
    "        num_edges, edge_feat_dim = edge_feat.shape\n",
    "#         print(num_edges, edge_feat_dim)\n",
    "        \n",
    "        h_e = edge_feat\n",
    "        \n",
    "        # concat 3 features\n",
    "        test = torch.cat([h_u, h_v, h_e],1)\n",
    "#         print(\"with edge: \", test.shape)\n",
    "        \n",
    "        test = torch.cat([h_u, h_v],1)\n",
    "#         print(\"without edge: \", test.shape)\n",
    "        \n",
    "        score = self.W(torch.cat([h_u, h_v, h_e], 1))\n",
    "#         score = self.W(torch.cat([h_u, h_v], 1))\n",
    "\n",
    "        return {'score': score}\n",
    "\n",
    "\n",
    "    def forward(self, graph, new_node_feat, edge_feat):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['new_node_feat'] = new_node_feat\n",
    "#             graph.apply_edges(self.apply_edges)\n",
    "\n",
    "            # 在 apply_edges 时传递 edge_feat\n",
    "            graph.apply_edges(lambda edges: self.apply_edges(edges, edge_feat))\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_classes, edge_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.sage = GraphSAGE(in_features, hidden_features, out_features)\n",
    "        self.pred = MLPPredictor(out_features, num_classes, edge_embedding_dim)\n",
    "      \n",
    "    def forward(self, g, node_feat, edge_feat, return_logits=False):\n",
    "        new_node_feat = self.sage(g, node_feat)\n",
    "        logits = self.pred(g, new_node_feat, edge_feat)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Forward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(batched_g, model, criterion, device, count=1, which_type='train'):\n",
    "    \"\"\"Forward a batch through the model.\"\"\"\n",
    "#     batched_g, labels = data\n",
    "    batched_g = batched_g.to(device)\n",
    "    \n",
    "    labels = batched_g.edata['label'].to(device)\n",
    "    \n",
    "#     logits = model(batched_g, batched_g.ndata['feat'].float())\n",
    "    logits = model(batched_g, batched_g.ndata['feat'].float(), batched_g.edata['feat'].float())\n",
    "\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    output = torch.softmax(logits, dim=1)\n",
    "    preds = output.argmax(1)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = torch.mean((preds == labels).float())\n",
    "        \n",
    "    return loss, accuracy, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For release the GPU memory\n",
    "    - no need to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For release the GPU memory\n",
    "# # No need to restart the kernel\n",
    "\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8787\n",
    "in_dim = DIM # dimension of the node feature\n",
    "hidden_dim = 64\n",
    "out_dim = 128\n",
    "num_classes = 2 # for DARPA\n",
    "edge_dim = edge_embedding_dim\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "total_steps = 100\n",
    "patience = 5\n",
    "waiting = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(in_dim, hidden_dim, out_dim, num_classes, edge_dim)\n",
    "best_model_path = \"./checkpoint_graphSAGE/best_model_GraphSAGE_transE_100-plusedge.pt\"\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr)\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=36, eta_min=0, last_epoch=- 1, verbose=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "same_seeds(seed)\n",
    "model = model.to(device)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training Part\n",
    "for epoch in tqdm(range(total_steps)):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batched_g in tqdm(dataloaders['train'], desc=\"Training\", position=0, leave=True):\n",
    "        num_batches += 1\n",
    "        loss, accuracy, _ = model_fn(batched_g, model, criterion, device, num_batches, which_type='train')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    add_log_msg(f\"total batches: {num_batches}\")\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "\n",
    "    add_log_msg(f'Epoch {epoch} | Train Loss: {avg_loss:.4f} | Train Accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "    \n",
    "    # Validation Part\n",
    "    model.eval()\n",
    "    total_accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batched_g in tqdm(dataloaders['valid'], desc=\"Validation\", position=0, leave=True):\n",
    "            loss, accuracy, _ = model_fn(batched_g, model, criterion, device, num_batches, which_type='validation')\n",
    "            total_accuracy += accuracy.item()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    current_loss = total_loss / num_batches\n",
    "    \n",
    "    add_log_msg(f'Validation Loss: {current_loss:.4f} | Validation Accuracy: {avg_accuracy:.4f}\\n')\n",
    "    \n",
    "            \n",
    "    if current_loss < best_val_loss:\n",
    "        best_val_loss = current_loss\n",
    "        waiting = 0\n",
    "        \n",
    "        if os.path.exists(best_model_path):\n",
    "            os.remove(best_model_path)\n",
    "            add_log_msg(\"Find a better model!!\")\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    else:\n",
    "        waiting += 1\n",
    "        if waiting >= patience:\n",
    "            add_log_msg(\"============================== Early stopping ==================================\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batched_g in tqdm(dataloaders['test'], desc=\"Testing\", position=0, leave=True):\n",
    "#         print(f\"data:{data[1]}\")\n",
    "        loss, accuracy, predicted = model_fn(batched_g, model, criterion, device, count, which_type='test')\n",
    "        labels = batched_g.edata['label'].to(device)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        # if count % 5000 == 0:\n",
    "        #     add_log_msg(f\"labels: {labels} {labels.shape}\")\n",
    "        #     add_log_msg(f\"predicted: {predicted} {predicted.shape}\")\n",
    "            \n",
    "        count += 1\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "add_log_msg(f'Test Accuracy: {100 * correct / total} %\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fix the seed and save the model.state_dict that contains the initial weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8787\n",
    "same_seeds(seed)\n",
    "\n",
    "model = Model(in_features=50, hidden_features=64, out_features=128, num_classes=167)\n",
    "torch.save(model.state_dict(), 'model3_initial(graphsage)/initial_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0181, -0.0857,  0.1973,  ...,  0.2417,  0.2702, -0.3041],\n",
       "        [-0.0768, -0.2723, -0.2001,  ...,  0.2989, -0.1387, -0.1940],\n",
       "        [ 0.2582, -0.0822,  0.3086,  ..., -0.0257, -0.1119, -0.0335],\n",
       "        ...,\n",
       "        [ 0.2274, -0.0411, -0.0334,  ..., -0.1679,  0.2455,  0.2424],\n",
       "        [ 0.1375,  0.2813,  0.0775,  ...,  0.1337,  0.2065,  0.2618],\n",
       "        [-0.0951,  0.1010, -0.2586,  ..., -0.1242, -0.0631,  0.0924]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.layer1.fc_self.weight\n",
    "model.sage.layer1.fc_self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if model really load the model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0181, -0.0857,  0.1973,  ...,  0.2417,  0.2702, -0.3041],\n",
       "        [-0.0768, -0.2723, -0.2001,  ...,  0.2989, -0.1387, -0.1940],\n",
       "        [ 0.2582, -0.0822,  0.3086,  ..., -0.0257, -0.1119, -0.0335],\n",
       "        ...,\n",
       "        [ 0.2274, -0.0411, -0.0334,  ..., -0.1679,  0.2455,  0.2424],\n",
       "        [ 0.1375,  0.2813,  0.0775,  ...,  0.1337,  0.2065,  0.2618],\n",
       "        [-0.0951,  0.1010, -0.2586,  ..., -0.1242, -0.0631,  0.0924]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(in_features=50, hidden_features=64, out_features=128, num_classes=167)\n",
    "model.load_state_dict(torch.load('model3_initial(graphsage)/initial_weight.pth'))\n",
    "model.sage.layer1.fc_self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file = '../data/label2id.txt'\n",
    "label_mapping = {}\n",
    "with open(mapping_file, 'r') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ')\n",
    "        label_mapping[int(parts[1])] = parts[0]\n",
    "        \n",
    "# 将映射后的标签应用到true和predicted标签列表\n",
    "mapped_true_labels = [label_mapping[label] for label in true_labels]\n",
    "mapped_predicted_labels = [label_mapping[label] for label in predicted_labels]\n",
    "\n",
    "# 生成Scikit-learn报告信息的DataFrame\n",
    "report_data = classification_report(mapped_true_labels, mapped_predicted_labels, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "\n",
    "# mapped_true_labels_np = np.array(mapped_true_labels)\n",
    "# mapped_predicted_labels_np = np.array(mapped_predicted_labels)\n",
    "\n",
    "# print(\"mapped_true_labels 的形状:\", mapped_true_labels_np.shape)\n",
    "# print(\"mapped_predicted_labels 的形状:\", mapped_predicted_labels_np.shape)\n",
    "\n",
    "report_folder = 'classification_report'\n",
    "os.makedirs(report_folder, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    report_filename = f'classification_report-transE_50-graphSAGE-{count}.xlsx'\n",
    "    labels_filename = f'mapped_true_predicted_labels-transE_50-graphSAGE-{count}.xlsx'\n",
    "    \n",
    "    report_path = os.path.join(report_folder, report_filename)\n",
    "    labels_path = os.path.join(report_folder, labels_filename)\n",
    "    \n",
    "    if not os.path.exists(report_path) and not os.path.exists(labels_path):\n",
    "        break\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "report_df.to_excel(report_path, index_label='Label')\n",
    "\n",
    "mapped_labels_df = pd.DataFrame({'true_label': mapped_true_labels, 'predicted_label': mapped_predicted_labels})\n",
    "mapped_labels_df.to_excel(labels_path, index=False)\n",
    "\n",
    "add_log_msg(f\"report path: {report_path}\")\n",
    "add_log_msg(f\"label path: {labels_path}\")\n",
    "\n",
    "mapped_report = classification_report(mapped_true_labels, mapped_predicted_labels)\n",
    "add_log_msg(f\"mapped_report:\\n{mapped_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_data = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "# report_df = pd.DataFrame(report_data).transpose()\n",
    "\n",
    "# report_folder = 'classification_report'\n",
    "# os.makedirs(report_folder, exist_ok=True)\n",
    "\n",
    "# count = 0\n",
    "# while True:\n",
    "#     report_filename = f'classification_report-transE_50-graphSAGE-{count}.xlsx'\n",
    "#     labels_filename = f'mapped_true_predicted_labels-transE_50-graphSAGE-{count}.xlsx'\n",
    "    \n",
    "#     report_path = os.path.join(report_folder, report_filename)\n",
    "#     labels_path = os.path.join(report_folder, labels_filename)\n",
    "    \n",
    "#     if not os.path.exists(report_path) and not os.path.exists(labels_path):\n",
    "#         break\n",
    "#     count += 1\n",
    "\n",
    "    \n",
    "# report_df.to_excel(report_path, index_label='Label')\n",
    "\n",
    "# labels_df = pd.DataFrame({'true_label': true_labels, 'predicted_label': predicted_labels})\n",
    "# labels_df.to_excel(labels_path, index=False)\n",
    "\n",
    "# add_log_msg(f\"report path: {report_path}\")\n",
    "# add_log_msg(f\"label path: {labels_path}\")\n",
    "\n",
    "# report = classification_report(true_labels, predicted_labels)\n",
    "# add_log_msg(f\"report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
