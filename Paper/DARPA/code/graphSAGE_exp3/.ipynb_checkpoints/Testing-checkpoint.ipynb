{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124662a9-3572-4fb9-8f5e-a3735d63205f",
   "metadata": {},
   "source": [
    "## For only Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b6bb36-e683-4031-b41a-712a33e0985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dgl\n",
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import subprocess\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from dgl.nn import GraphConv, GATConv, SAGEConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb93b58-b6b2-4b88-bf92-eb5978efec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# def get_free_gpu():\n",
    "#     try:\n",
    "#         # Run nvidia-smi command to get GPU details\n",
    "#         _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "#         command = \"nvidia-smi --query-gpu=memory.free --format=csv,nounits,noheader\"\n",
    "#         memory_free_info = _output_to_list(subprocess.check_output(command.split())) \n",
    "#         memory_free_values = [int(x) for i, x in enumerate(memory_free_info)]\n",
    "        \n",
    "#         # Get the GPU with the maximum free memory\n",
    "#         best_gpu_id = memory_free_values.index(max(memory_free_values))\n",
    "#         return best_gpu_id\n",
    "#     except:\n",
    "#         # If any exception occurs, default to GPU 0 (this handles cases where nvidia-smi isn't installed)\n",
    "#         return 0\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     # Get the best GPU ID based on free memory and set it\n",
    "#     best_gpu_id = get_free_gpu()\n",
    "#     device = torch.device(f\"cuda:{best_gpu_id}\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"there's no available GPU\")\n",
    "\n",
    "device = torch.device(f\"cuda:{0}\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe143bc-7760-40c2-91a9-e6b2ced15399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix seed\n",
    "def same_seeds(seed = 8787):\n",
    "    torch.manual_seed(seed)\n",
    "    # random.seed(seed) \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf891e3-3f9f-4172-bd84-69509c5a06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 50\n",
    "embedding = \"transR\"\n",
    "embedding = f'{embedding}_{DIM}'\n",
    "\n",
    "with open(f\"../../data/4_embedding/synthesize/{embedding}.vec.json\", \"r\") as f:\n",
    "    tmp = json.load(f)\n",
    "\n",
    "index2entemb = {idx:emb for idx, emb in enumerate(tmp[\"ent_embeddings.weight\"])}\n",
    "index2relemb = {idx:emb for idx, emb in enumerate(tmp[\"rel_embeddings.weight\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f7163f-2d6b-40cf-bcf5-3adab8a27aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55eaa92576104ac6896181bb2473fb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading:   0%|          | 0/399000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH...\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/source_data/before_embedding/3.10/all_graph_data.jsonl\", \"r\") as f:\n",
    "    print(\"Loading the data...\")\n",
    "#     input_data = list(f)\n",
    "#     input_data = [json.loads(line) for idx, line in tqdm(f, desc=\"Loading\")]\n",
    "\n",
    "    # only process 40000 data from 400000 data\n",
    "    wanted_data = 399000\n",
    "    input_data = []\n",
    "    for idx, line in tqdm(enumerate(f), total=wanted_data, desc=\"Loading\"):\n",
    "        if idx == wanted_data:\n",
    "            break\n",
    "        input_data.append(json.loads(line))\n",
    "        \n",
    "    print(\"FINISH...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "183aabe7-479c-4342-b781-773ed1896a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546031873ed84d7684f92b69208c4aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/399000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============ If type(input_data[0] == dict) ============\n",
    "for data_point in tqdm(input_data):\n",
    "    data_point['node_feat'] = [index2entemb[node_id] for node_id in data_point['node_feat']]\n",
    "    data_point['edge_attr'] = [index2relemb[edge_id] for edge_id in data_point['edge_attr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef495c9-defd-45ad-a9ca-0091e91d853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_list, device):\n",
    "        self.data_list = data_list\n",
    "        self.device = device\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "        return data\n",
    "\n",
    "def collate(samples):\n",
    "    data_list = samples\n",
    "    batched_graphs = []\n",
    "    for data in data_list:\n",
    "        g = dgl.graph((th.tensor(data[\"edge_index\"][0]), th.tensor(data[\"edge_index\"][1])), num_nodes=data[\"num_nodes\"])\n",
    "\n",
    "        g.ndata['feat'] = th.tensor(data[\"node_feat\"])\n",
    "        g.edata['feat'] = th.tensor(data[\"edge_attr\"])\n",
    "\n",
    "        g.edata['label'] = th.tensor(data['labels'])\n",
    "\n",
    "        batched_graphs.append(g)\n",
    "    \n",
    "    return dgl.batch(batched_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d45226d9-00b5-4382-84ea-cc089816ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded and ready for training!\n"
     ]
    }
   ],
   "source": [
    "total_data = len(input_data)\n",
    "\n",
    "test_size = int(total_data * 0.1)\n",
    "train_valid_size = total_data - test_size\n",
    "\n",
    "train_valid_data = input_data[:train_valid_size]\n",
    "test_data = input_data[train_valid_size:]\n",
    "\n",
    "train_data, valid_data = train_test_split(train_valid_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# creating GraphDataset\n",
    "dataset_data = {\n",
    "    'train': GraphDataset(train_data, device),\n",
    "    'valid': GraphDataset(valid_data, device),\n",
    "    'test': GraphDataset(test_data, device)\n",
    "}\n",
    "\n",
    "print(\"Datasets loaded and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf76fbfe-7749-4af6-8152-be5944a5c43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded and ready for training!\n"
     ]
    }
   ],
   "source": [
    "# # split 8:1:1 (train, valid, test)\n",
    "# train_data, remaining_data = train_test_split(input_data, test_size=0.2, random_state=42)\n",
    "# valid_data, test_data = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# dataset_data = {\n",
    "#     'train': GraphDataset(train_data, device),\n",
    "#     'valid': GraphDataset(valid_data, device),\n",
    "#     'test': GraphDataset(test_data, device)\n",
    "# }\n",
    "\n",
    "# print(\"Datasets loaded and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc9482-aaa8-4e43-95d2-71650b86c88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59b238ca-d3ab-469b-9552-f9fa61be8d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39900"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e182cc-4a9c-44b8-9e9d-0822c38afe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(batch_size, shuffle=True):\n",
    "    dataloaders = {}\n",
    "    for dataset_name, dataset in dataset_data.items():\n",
    "        # do not shuffle the testing dataset\n",
    "        if dataset_name == \"test\":\n",
    "            dataloaders[dataset_name] = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)    \n",
    "        else:\n",
    "            dataloaders[dataset_name] = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate)\n",
    "    return dataloaders\n",
    "\n",
    "dataloaders = create_dataloaders(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962231c-7784-476a-ae6c-7cacc18a3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "formatted_time = now.strftime(\"%m%d_%H:%M\")\n",
    "\n",
    "log_file_path = f\"./log_message/{formatted_time}_GraphSAGE_{embedding}-testing.log\"\n",
    "\n",
    "def add_log_msg(msg, log_file_path=log_file_path):\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        f.write(f'{datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}# {msg}\\n')\n",
    "    print(f'{datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}# {msg}')\n",
    "\n",
    "print(log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84996b-4a13-4d87-95f0-9a6639e8dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layer1 = dglnn.SAGEConv(in_dim, hidden_dim, 'pool')\n",
    "        self.layer2 = dglnn.SAGEConv(hidden_dim, out_dim, 'pool')\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.layer1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.layer2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6af934-c3e4-4be3-b04c-88cc2a65fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, out_feats, out_classes):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(out_feats*2, out_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h_u = edges.src['h']\n",
    "        h_v = edges.dst['h']\n",
    "        score = self.W(torch.cat([h_u, h_v], 1))\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(self.apply_edges)\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47994d21-67e1-4bb5-bce1-783b8a5b8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.sage = GraphSAGE(in_features, hidden_features, out_features)\n",
    "        self.pred = MLPPredictor(out_features, num_classes)\n",
    "      \n",
    "    def forward(self, g, node_feat, return_logits=False):\n",
    "        h = self.sage(g, node_feat)\n",
    "        logits = self.pred(g, h)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870df35c-1b64-4921-bf7f-054af75996f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(batched_g, model, criterion, device, count=1, which_type='train'):\n",
    "    \"\"\"Forward a batch through the model.\"\"\"\n",
    "#     batched_g, labels = data\n",
    "    batched_g = batched_g.to(device)\n",
    "    \n",
    "    labels = batched_g.edata['label'].to(device)\n",
    "    \n",
    "    logits = model(batched_g, batched_g.ndata['feat'].float())\n",
    "\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    output = torch.softmax(logits, dim=1)\n",
    "    preds = output.argmax(1)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = torch.mean((preds == labels).float())\n",
    "        \n",
    "    return loss, accuracy, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46804376-e748-4a1f-bd61-29af0f43ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5269\n",
    "in_dim = DIM # dimension of the node feature\n",
    "hidden_dim = 64\n",
    "out_dim = 128\n",
    "num_classes = 2 # for DARPA\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "total_steps = 100\n",
    "patience = 5\n",
    "waiting = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74571168-7802-4a0d-a67e-7a8cbc9f7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(in_dim, hidden_dim, out_dim, num_classes)\n",
    "best_model_path = f\"./checkpoint_graphSAGE/best_model_GraphSAGE_{embedding}.pt\"\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr)\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e6c3b-f62d-4519-bebc-e58f7fcdd9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batched_g in tqdm(dataloaders['test'], desc=\"Testing\", position=0, leave=True):\n",
    "#         print(f\"data:{data[1]}\")\n",
    "        loss, accuracy, predicted = model_fn(batched_g, model, criterion, device, count, which_type='test')\n",
    "        labels = batched_g.edata['label'].to(device)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "            \n",
    "        count += 1\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "add_log_msg(f'Test Accuracy: {100 * correct / total} %\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd42c2-6281-4e7b-8341-ce3b97017fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26635568-ff41-4f5e-810d-d89dc07d513d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b506264-487f-4da9-a304-d2fbc4e71b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
