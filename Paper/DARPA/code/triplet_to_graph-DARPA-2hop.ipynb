{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "print('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the file to chanage the original merged tripelt.txt to 2-hop graph\n",
    "- use BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, deque\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Final version that can get all the 2-hop neighbors \n",
    "- Not merged the grpah yet\n",
    "- the version of directly output the relation [......]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_lines):\n",
    "    connections = defaultdict(dict)\n",
    "    node_labels = {}\n",
    "\n",
    "    for line in tqdm(data_lines, desc=\"Processing data lines\"):\n",
    "        parts = line.strip().split()\n",
    "        src, dest, edge_info, label = int(parts[0]), int(parts[1]), parts[2], parts[3]\n",
    "\n",
    "        connections[src][dest] = (edge_info, label)\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label\n",
    "\n",
    "    return connections, node_labels\n",
    "\n",
    "\n",
    "def bfs_2hop(connections, start_node):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node, 0)])  # Queue will store a tuple of (node, hops)\n",
    "\n",
    "    while queue:\n",
    "        current_node, hops = queue.popleft()\n",
    "        visited.add(current_node)\n",
    "\n",
    "        if hops < 2:  # Only proceed if we are within 2 hops\n",
    "            # Check for neighbors where the current node is the source\n",
    "            for neigh in connections[current_node]:\n",
    "                if neigh not in visited:\n",
    "                    queue.append((neigh, hops + 1))\n",
    "\n",
    "            # Check for neighbors where the current node is the destination\n",
    "            for src, dests in connections.items():\n",
    "                if current_node in dests and src not in visited:\n",
    "                    queue.append((src, hops + 1))\n",
    "\n",
    "    return visited - {start_node}  # Exclude the start_node itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the version of getting the data with dictionary:\n",
    "- source node: [......]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_lines):\n",
    "    connections = defaultdict(dict)\n",
    "    node_labels = {}\n",
    "\n",
    "    for line in tqdm(data_lines, desc=\"Processing data lines\"):\n",
    "        parts = line.strip().split()\n",
    "        src, dest, edge_info, label = int(parts[0]), int(parts[1]), parts[2], parts[3]\n",
    "\n",
    "        connections[src][dest] = (edge_info, label)\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label\n",
    "\n",
    "    return connections, node_labels\n",
    "\n",
    "\n",
    "def bfs_2hop(connections, start_node):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node, 0)])  # Queue will store a tuple of (node, hops)\n",
    "\n",
    "    while queue:\n",
    "        current_node, hops = queue.popleft()\n",
    "        visited.add(current_node)\n",
    "\n",
    "        if hops < 2:  # Only proceed if we are within 2 hops\n",
    "            # Check for neighbors where the current node is the source\n",
    "            for neigh in connections[current_node]:\n",
    "                if neigh not in visited:\n",
    "                    queue.append((neigh, hops + 1))\n",
    "\n",
    "            # Check for neighbors where the current node is the destination\n",
    "            for src, dests in connections.items():\n",
    "                if current_node in dests and src not in visited:\n",
    "                    queue.append((src, hops + 1))\n",
    "\n",
    "    return visited - {start_node}  # Exclude the start_node itselfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subgraphs(connections, node_labels, label_type):\n",
    "    subgraphs = {}\n",
    "\n",
    "    for node, label in tqdm(node_labels.items(), desc=f\"Extracting {label_type} subgraphs\"):\n",
    "        if label == label_type:\n",
    "            neighbors_2hop = bfs_2hop(connections, node)\n",
    "            subgraph = []\n",
    "\n",
    "            for neigh in neighbors_2hop.union({node}):  # 包括节点本身\n",
    "                if neigh in connections:  # 检查邻居是否有任何连接\n",
    "                    for dest, (edge_info, edge_label) in connections[neigh].items():\n",
    "                        if dest in neighbors_2hop or dest == node:  # 包括2-hop内的边或返回源节点\n",
    "                            subgraph_line = f\"{neigh} {dest} {edge_info} {edge_label}\"\n",
    "                            subgraph.append(subgraph_line)\n",
    "\n",
    "            if subgraph:  # 仅当子图不为空时添加\n",
    "                subgraphs[node] = subgraph  # 使用源节点作为键\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "# subgraphs_attack = extract_subgraphs(connections, node_labels, 'attack')\n",
    "# subgraphs_benign = extract_subgraphs(connections, node_labels, 'benign')\n",
    "\n",
    "\n",
    "# print(subgraphs_benign, len(subgraphs_benign), '\\n\\n')\n",
    "# print(subgraphs_attack, len(subgraphs_attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_subgraphs(subgraphs_dict):\n",
    "    all_edges = set()\n",
    "    for edges in subgraphs_dict.values():\n",
    "        all_edges.update(edges)\n",
    "\n",
    "    merged_subgraphs = []\n",
    "    graph_map = {}  # A mapping of node to graph index in merged_subgraphs\n",
    "\n",
    "    for edge in tqdm(all_edges, desc=\"Merging subgraphs\"):\n",
    "        src, dest, *_ = edge.split()\n",
    "\n",
    "        src_graph = graph_map.get(src)\n",
    "        dest_graph = graph_map.get(dest)\n",
    "\n",
    "        if src_graph is not None and dest_graph is not None:\n",
    "            if src_graph != dest_graph:\n",
    "                # Merge the two graphs\n",
    "                merged_subgraphs[src_graph].update(merged_subgraphs[dest_graph])\n",
    "                merged_subgraphs[dest_graph] = merged_subgraphs[src_graph]\n",
    "                # Update the graph_map for all nodes in the merged graph\n",
    "                for node in merged_subgraphs[src_graph]:\n",
    "                    graph_map[node] = src_graph\n",
    "            merged_subgraphs[src_graph].add(edge)\n",
    "        elif src_graph is not None:\n",
    "            # Add to src graph\n",
    "            merged_subgraphs[src_graph].add(edge)\n",
    "            graph_map[dest] = src_graph\n",
    "        elif dest_graph is not None:\n",
    "            # Add to dest graph\n",
    "            merged_subgraphs[dest_graph].add(edge)\n",
    "            graph_map[src] = dest_graph\n",
    "        else:\n",
    "            # Create a new graph\n",
    "            new_index = len(merged_subgraphs)\n",
    "            merged_subgraphs.append({edge})\n",
    "            graph_map[src] = graph_map[dest] = new_index\n",
    "\n",
    "    # Remove duplicate graphs\n",
    "    unique_subgraphs = []\n",
    "    for subgraph in merged_subgraphs:\n",
    "        if subgraph not in unique_subgraphs:\n",
    "            unique_subgraphs.append(subgraph)\n",
    "\n",
    "    return unique_subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c7ab3caf1740c2848d3a79c5644dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging subgraphs:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'10 1000 2 benign',\n",
       "  '1000 500 1 benign',\n",
       "  '24027 8575 14 benign',\n",
       "  '24027 8787 10 benign',\n",
       "  '24462 8575 2 benign',\n",
       "  '8575 10 2 benign'},\n",
       " {'1 3 5 benign'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_graphs = merge_overlapping_subgraphs(subgraphs_benign)\n",
    "merged_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef72951ebb0e4ed08e2ab0a01808615c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging subgraphs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'16301 10385 6 attack'}, {'16301 2 100 benign'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_graphs = merge_overlapping_subgraphs(subgraphs_attack)\n",
    "merged_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_subgraphs(subgraphs_dict):\n",
    "    all_edges = set()\n",
    "    for edges in subgraphs_dict.values():\n",
    "        all_edges.update(edges)\n",
    "\n",
    "    merged_subgraphs = []\n",
    "    graph_map = {}  # A mapping of node to graph index in merged_subgraphs\n",
    "\n",
    "    for edge in tqdm(all_edges, desc=\"Merging subgraphs\"):\n",
    "        src, dest, *_ = edge.split()\n",
    "\n",
    "        src_graph = graph_map.get(src)\n",
    "        dest_graph = graph_map.get(dest)\n",
    "\n",
    "        if src_graph is not None:\n",
    "            # Add to src graph\n",
    "            merged_subgraphs[src_graph].add(edge)\n",
    "            graph_map[dest] = src_graph\n",
    "        elif dest_graph is not None:\n",
    "            # Add to dest graph\n",
    "            merged_subgraphs[dest_graph].add(edge)\n",
    "            graph_map[src] = dest_graph\n",
    "        else:\n",
    "            # Create a new graph\n",
    "            new_index = len(merged_subgraphs)\n",
    "            merged_subgraphs.append({edge})\n",
    "            graph_map[src] = graph_map[dest] = new_index\n",
    "\n",
    "    # Remove duplicate graphs\n",
    "    unique_subgraphs = []\n",
    "    for subgraph in merged_subgraphs:\n",
    "        if subgraph not in unique_subgraphs:\n",
    "            unique_subgraphs.append(subgraph)\n",
    "\n",
    "    return unique_subgraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f1c55cac194eac93968b65e19d1203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging subgraphs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'16301 10385 6 attack', '16301 2 100 benign'}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_graphs = merge_overlapping_subgraphs(subgraphs_attack)\n",
    "print(len(merged_graphs))\n",
    "merged_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3468d459e6f4d108c45fa226f9ef0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging subgraphs:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'10 1000 2 benign',\n",
       "  '1000 500 1 benign',\n",
       "  '24027 8575 14 benign',\n",
       "  '24027 8787 10 benign',\n",
       "  '24462 8575 2 benign',\n",
       "  '8575 10 2 benign'},\n",
       " {'1 3 5 benign'}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_graphs = merge_overlapping_subgraphs(subgraphs_benign)\n",
    "print(len(merged_graphs))\n",
    "merged_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merged_subgraphs_to_files(merged_subgraphs, output_folder, label_type):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for i, subgraph in enumerate(tqdm(merged_subgraphs, desc=f\"Saving {label_type} merged subgraphs\")):\n",
    "        file_name = os.path.join(output_folder, f\"{label_type}_merged_subgraph_{i}.txt\")\n",
    "        with open(file_name, 'w') as file:\n",
    "            for edge in subgraph:\n",
    "                file.write(f\"{edge}\\n\")  # 直接写入边的字符串\n",
    "        print(f\"{file_name} saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_graphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/source_data/output_graphs(200)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m save_merged_subgraphs_to_files(\u001b[43mmerged_graphs\u001b[49m, output_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenign\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_graphs' is not defined"
     ]
    }
   ],
   "source": [
    "output_folder = '../data/source_data/output_graphs(200)'\n",
    "\n",
    "save_merged_subgraphs_to_files(merged_graphs, output_folder, 'benign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cb028e32194fc4ae89361903c73772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data lines:   0%|          | 0/51479769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m input_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/source_data/merged_all.txt\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update with your actual file path\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_lines \u001b[38;5;241m=\u001b[39m read_data_from_file(input_file_path)\n\u001b[0;32m----> 3\u001b[0m connections, labels, label_to_nodes \u001b[38;5;241m=\u001b[39m process_data(data_lines)\n\u001b[1;32m      5\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/source_data/output_graphs(200)\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update with your actual output folder path\u001b[39;00m\n\u001b[1;32m      7\u001b[0m subgraphs_benign \u001b[38;5;241m=\u001b[39m extract_subgraphs(connections, label_to_nodes, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenign\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "input_file_path = './data/source_data/merged_all.txt'  # Update with your actual file path\n",
    "data_lines = read_data_from_file(input_file_path)\n",
    "connections, labels, label_to_nodes = process_data(data_lines)\n",
    "\n",
    "output_folder = './data/source_data/output_graphs(200)'  # Update with your actual output folder path\n",
    "\n",
    "subgraphs_benign = extract_subgraphs(connections, label_to_nodes, 'benign', 200)\n",
    "subgraphs_attack = extract_subgraphs(connections, label_to_nodes, 'attack', 200)\n",
    "\n",
    "merged_subgraphs_benign = merge_overlapping_subgraphs(subgraphs_benign)\n",
    "merged_subgraphs_attack = merge_overlapping_subgraphs(subgraphs_attack)\n",
    "\n",
    "save_merged_subgraphs_to_files(merged_subgraphs_benign, output_folder, 'benign')\n",
    "save_merged_subgraphs_to_files(merged_subgraphs_attack, output_folder, 'attack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the complete code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 先建立大圖\n",
    "    - networkx 的neighbor function 試試看\n",
    "    \n",
    "- 讀全部tuple進來\n",
    "    - 建立set of source destination node sets\n",
    "    - node 2 entity dictionary for neighbor\n",
    "    - node 2 event list\n",
    "    - attack 優先算\n",
    "    - networkx 最後一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, deque\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        print(f\"read file from: {file_path}\")\n",
    "        return file.readlines()\n",
    "\n",
    "def process_data(data_lines):\n",
    "    connections = defaultdict(dict)\n",
    "    node_labels = {}\n",
    "\n",
    "    for line in tqdm(data_lines, desc=\"Processing data lines\"):\n",
    "        parts = line.strip().split()\n",
    "        src, dest, edge_info, label = int(parts[0]), int(parts[1]), parts[2], parts[3]\n",
    "\n",
    "        connections[src][dest] = (edge_info, label)\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label\n",
    "\n",
    "    return connections, node_labels\n",
    "\n",
    "\n",
    "def bfs_2hop(connections, start_node):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node, 0)])  # Queue will store a tuple of (node, hops)\n",
    "\n",
    "    while queue:\n",
    "        current_node, hops = queue.popleft()\n",
    "        visited.add(current_node)\n",
    "\n",
    "        if hops < 2:  # Only proceed if we are within 2 hops\n",
    "            # Check for neighbors where the current node is the source\n",
    "            for neigh in connections[current_node]:\n",
    "                if neigh not in visited:\n",
    "                    queue.append((neigh, hops + 1))\n",
    "\n",
    "            # Check for neighbors where the current node is the destination\n",
    "            for src, dests in connections.items():\n",
    "                if current_node in dests and src not in visited:\n",
    "                    queue.append((src, hops + 1))\n",
    "\n",
    "    return visited - {start_node}  # Exclude the start_node itself\n",
    "\n",
    "\n",
    "def extract_subgraphs(connections, node_labels, label_type):\n",
    "    subgraphs = {}\n",
    "\n",
    "#     for node, label in tqdm(node_labels.items(), desc=f\"Extracting {label_type} subgraphs\"):\n",
    "    for node, label in node_labels.items():\n",
    "        if label == label_type:\n",
    "            neighbors_2hop = bfs_2hop(connections, node)\n",
    "            subgraph = []\n",
    "\n",
    "            for neigh in neighbors_2hop.union({node}):  # 包括节点本身\n",
    "                if neigh in connections:  # 检查邻居是否有任何连接\n",
    "                    for dest, (edge_info, edge_label) in connections[neigh].items():\n",
    "                        if dest in neighbors_2hop or dest == node:  # 包括2-hop内的边或返回源节点\n",
    "                            subgraph_line = f\"{neigh} {dest} {edge_info} {edge_label}\"\n",
    "                            subgraph.append(subgraph_line)\n",
    "\n",
    "            if subgraph:  # 仅当子图不为空时添加\n",
    "                subgraphs[node] = subgraph  # 使用源节点作为键\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "\n",
    "def merge_overlapping_subgraphs(subgraphs_dict):\n",
    "    all_edges = set()\n",
    "    for edges in subgraphs_dict.values():\n",
    "        all_edges.update(edges)\n",
    "\n",
    "    merged_subgraphs = []\n",
    "    graph_map = {}  # A mapping of node to graph index in merged_subgraphs\n",
    "\n",
    "    for edge in tqdm(all_edges, desc=\"Merging subgraphs\"):\n",
    "        src, dest, *_ = edge.split()\n",
    "\n",
    "        src_graph = graph_map.get(src)\n",
    "        dest_graph = graph_map.get(dest)\n",
    "\n",
    "        if src_graph is not None and dest_graph is not None:\n",
    "            if src_graph != dest_graph:\n",
    "                # Merge the two graphs\n",
    "                merged_subgraphs[src_graph].update(merged_subgraphs[dest_graph])\n",
    "                merged_subgraphs[dest_graph] = merged_subgraphs[src_graph]\n",
    "                # Update the graph_map for all nodes in the merged graph\n",
    "                for node in merged_subgraphs[src_graph]:\n",
    "                    graph_map[node] = src_graph\n",
    "            merged_subgraphs[src_graph].add(edge)\n",
    "        elif src_graph is not None:\n",
    "            # Add to src graph\n",
    "            merged_subgraphs[src_graph].add(edge)\n",
    "            graph_map[dest] = src_graph\n",
    "        elif dest_graph is not None:\n",
    "            # Add to dest graph\n",
    "            merged_subgraphs[dest_graph].add(edge)\n",
    "            graph_map[src] = dest_graph\n",
    "        else:\n",
    "            # Create a new graph\n",
    "            new_index = len(merged_subgraphs)\n",
    "            merged_subgraphs.append({edge})\n",
    "            graph_map[src] = graph_map[dest] = new_index\n",
    "\n",
    "    # Remove duplicate graphs\n",
    "    unique_subgraphs = []\n",
    "    for subgraph in merged_subgraphs:\n",
    "        if subgraph not in unique_subgraphs:\n",
    "            unique_subgraphs.append(subgraph)\n",
    "\n",
    "    return unique_subgraphs\n",
    "\n",
    "\n",
    "def save_merged_subgraphs_to_files(merged_subgraphs, output_folder, label_type):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for i, subgraph in enumerate(tqdm(merged_subgraphs, desc=f\"Saving {label_type} merged subgraphs\")):\n",
    "        file_name = os.path.join(output_folder, f\"{label_type}_merged_subgraph_{i}.txt\")\n",
    "        with open(file_name, 'w') as file:\n",
    "            for edge in subgraph:\n",
    "                file.write(f\"{edge}\\n\")\n",
    "        print(f\"{file_name} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file from: /workdir/home/bai/Euni_HO_modified/data/source_data/merged_all_sorted.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaf9cf04acd4017ac44d9a227d26967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data lines:   0%|          | 0/51479769 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ecffc0a0c44767a0e89077e9334c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging subgraphs:   0%|          | 0/421185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912d34bd769a4cb49653ff1ce4a319d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging subgraphs:   0%|          | 0/426225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def select_random_sources(node_labels, label_type, count):\n",
    "    nodes = [node for node, label in node_labels.items() if label == label_type]\n",
    "    return random.sample(nodes, min(count, len(nodes)))\n",
    "\n",
    "def main_process(connections, node_labels, iteration, base_output_folder):\n",
    "    # choose random source nodes\n",
    "    benign_sources = select_random_sources(node_labels, 'benign', 200)\n",
    "    attack_sources = select_random_sources(node_labels, 'attack', 200)\n",
    "\n",
    "    # get subgraph\n",
    "    subgraphs_benign = {}\n",
    "    for src in benign_sources:\n",
    "        temp_subgraphs = extract_subgraphs(connections, {src: node_labels[src]}, 'benign')\n",
    "        subgraphs_benign.update(temp_subgraphs)\n",
    "\n",
    "    subgraphs_attack = {}\n",
    "    for src in attack_sources:\n",
    "        temp_subgraphs = extract_subgraphs(connections, {src: node_labels[src]}, 'attack')\n",
    "        subgraphs_attack.update(temp_subgraphs)\n",
    "\n",
    "    # merge the overlapping subgraph\n",
    "    merged_subgraphs_benign = merge_overlapping_subgraphs(subgraphs_benign)\n",
    "    merged_subgraphs_attack = merge_overlapping_subgraphs(subgraphs_attack)\n",
    "\n",
    "    # save into the output folder with different name\n",
    "    output_folder = f\"{base_output_folder}-{iteration}\"\n",
    "    save_merged_subgraphs_to_files(merged_subgraphs_benign, output_folder, 'benign')\n",
    "    save_merged_subgraphs_to_files(merged_subgraphs_attack, output_folder, 'attack')\n",
    "\n",
    "    \n",
    "input_file_path = '/workdir/home/bai/Euni_HO_modified/data/source_data/merged_all_sorted.txt'\n",
    "data_lines = read_data_from_file(input_file_path)\n",
    "connections, node_labels = process_data(data_lines)\n",
    "\n",
    "base_output_folder = '/workdir/home/bai/Euni_HO_modified/data/source_data/output_graphs(200)/output_graphs(200)'\n",
    "for i in range(10):\n",
    "    main_process(connections, node_labels, i, base_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = '/workdir/home/bai/Euni_HO_modified/data/source_data/merged_all.txt'  # Update with your actual file path\n",
    "data_lines = read_data_from_file(input_file_path)\n",
    "connections, labels, label_to_nodes = process_data(data_lines)\n",
    "\n",
    "output_folder = '/workdir/home/bai/Euni_HO_modified/data/source_data/output_graphs(200)'  # Update with your actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data lines:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting attack subgraphs:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting benign subgraphs:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['24027 8575 14 benign', '24027 8787 10 benign', '24462 8575 2 benign', '8575 10 2 benign'], ['24027 8575 14 benign', '24462 8575 2 benign', '8575 10 2 benign'], ['10 1000 2 benign', '24027 8575 14 benign', '24027 8787 10 benign', '24462 8575 2 benign', '8575 10 2 benign'], ['1000 500 1 benign', '10 1000 2 benign', '24027 8575 14 benign', '24462 8575 2 benign', '8575 10 2 benign'], ['1 3 5 benign'], ['1000 500 1 benign', '10 1000 2 benign', '8575 10 2 benign']] 6 \n",
      "\n",
      "\n",
      "[['16301 10385 6 attack', '16301 2 100 benign']] 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict, deque\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def process_data(data_lines):\n",
    "    connections = defaultdict(dict)\n",
    "    node_labels = {}\n",
    "\n",
    "    for line in tqdm(data_lines, desc=\"Processing data lines\"):\n",
    "        parts = line.strip().split()\n",
    "        src, dest, edge_info, label = int(parts[0]), int(parts[1]), parts[2], parts[3]\n",
    "\n",
    "        connections[src][dest] = (edge_info, label)\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label\n",
    "\n",
    "    return connections, node_labels\n",
    "\n",
    "# ================== not considering the backward direction ================== \n",
    "# def bfs_2hop(connections, start_node):\n",
    "#     visited = set()\n",
    "#     queue = deque([(start_node, 0)])  # Queue will store a tuple of (node, hops)\n",
    "\n",
    "#     while queue:\n",
    "#         current_node, hops = queue.popleft()\n",
    "#         visited.add(current_node)\n",
    "\n",
    "#         if hops < 2:  # Only proceed if we are within 2 hops\n",
    "#             for neigh in connections[current_node]:\n",
    "#                 if neigh not in visited:\n",
    "#                     queue.append((neigh, hops + 1))\n",
    "\n",
    "#     return visited - {start_node}  # Exclude the start_node itself\n",
    "\n",
    "def bfs_2hop(connections, start_node):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node, 0)])  # Queue will store a tuple of (node, hops)\n",
    "\n",
    "    while queue:\n",
    "        current_node, hops = queue.popleft()\n",
    "        visited.add(current_node)\n",
    "\n",
    "        if hops < 2:  # Only proceed if we are within 2 hops\n",
    "            # Check for neighbors where the current node is the source\n",
    "            for neigh in connections[current_node]:\n",
    "                if neigh not in visited:\n",
    "                    queue.append((neigh, hops + 1))\n",
    "\n",
    "            # Check for neighbors where the current node is the destination\n",
    "            for src, dests in connections.items():\n",
    "                if current_node in dests and src not in visited:\n",
    "                    queue.append((src, hops + 1))\n",
    "\n",
    "    return visited - {start_node}  # Exclude the start_node itself\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_subgraphs(connections, node_labels, label_type):\n",
    "    subgraphs = []\n",
    "\n",
    "    for node, label in tqdm(node_labels.items(), desc=f\"Extracting {label_type} subgraphs\"):\n",
    "        if label == label_type:\n",
    "#             neighbors_2hop = bfs_2hop(connections, node, set())  # 不再使用 visited_global\n",
    "            neighbors_2hop = bfs_2hop(connections, node)  # 不再使用 visited_global\n",
    "            subgraph = []\n",
    "\n",
    "            for neigh in neighbors_2hop.union({node}):  # 包括节点本身\n",
    "                if neigh in connections:  # 检查邻居是否有任何连接\n",
    "                    for dest, (edge_info, edge_label) in connections[neigh].items():\n",
    "                        if dest in neighbors_2hop or dest == node:  # 包括2-hop内的边或返回源节点\n",
    "                            subgraph_line = f\"{neigh} {dest} {edge_info} {edge_label}\"\n",
    "                            subgraph.append(subgraph_line)\n",
    "\n",
    "            if subgraph:  # 仅当子图不为空时添加\n",
    "                subgraphs.append(subgraph)\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "\n",
    "data_lines = [\n",
    "    \"24027 8575 14 benign\",\n",
    "    \"24027 8787 10 benign\",\n",
    "    \"24462 8575 2 benign\",\n",
    "    \"24462 8575 2 benign\",\n",
    "    \"8575 10 2 benign\",\n",
    "    \"10 1000 2 benign\",\n",
    "    \n",
    "    # the order would affect the output -> first see first output and into the visited nodes\n",
    "#     \"16301 2 100 benign\", \n",
    "    \"16301 10385 6 attack\",\n",
    "    \"16301 2 100 benign\",\n",
    "    \"1 3 5 benign\",\n",
    "    \"1000 500 1 benign\"\n",
    "]\n",
    "\n",
    "connections, node_labels = process_data(data_lines)\n",
    "subgraphs_attack = extract_subgraphs(connections, node_labels, 'attack')\n",
    "subgraphs_benign = extract_subgraphs(connections, node_labels, 'benign')\n",
    "\n",
    "\n",
    "print(subgraphs_benign, len(subgraphs_benign), '\\n\\n')\n",
    "print(subgraphs_attack, len(subgraphs_attack))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e92b95455a842d9b2a423d0bf714463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data lines:   0%|          | 0/51479769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3d7ce181fc4547862f98f8b4fd4dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting benign subgraphs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aded8906d0284aa0902d0f26e4ba41b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting attack subgraphs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4432f13e3abf44f8a455071c60812260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging subgraphs:   0%|          | 0/200 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78811fdd611144b18e079acb32c90478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging subgraphs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21b1f9e66054800bfd99e67bf20db2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving benign merged subgraphs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workdir/home/bai/Euni_HO_modified/data/source_data/output_graphs(200)/benign_merged_subgraph_0.txt saved\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_merged_sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-572dc73c91f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0msave_merged_subgraphs_to_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_subgraphs_benign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'benign'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0msave_merged_sub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'save_merged_sub' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict, deque\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def process_data(data_lines):\n",
    "    edges = [tuple(map(int, line.split()[:3])) for line in tqdm(data_lines, desc=\"Processing data lines\")]\n",
    "    connections = defaultdict(set)\n",
    "    labels = {}\n",
    "    label_to_nodes = {'benign': set(), 'attack': set()}\n",
    "\n",
    "    for src, dest, label in edges:\n",
    "        connections[src].add(dest)\n",
    "        connections[dest].add(src)\n",
    "        labels[src] = label\n",
    "        labels[dest] = label\n",
    "        if label == 0:\n",
    "            label_to_nodes['benign'].add(src)\n",
    "            label_to_nodes['benign'].add(dest)\n",
    "        elif label == 1:\n",
    "            label_to_nodes['attack'].add(src)\n",
    "            label_to_nodes['attack'].add(dest)\n",
    "\n",
    "    return connections, labels, label_to_nodes\n",
    "\n",
    "def bfs_2hop(connections, start_node):\n",
    "    visited = set()\n",
    "    queue = deque([start_node])\n",
    "    hops = {start_node: 0}\n",
    "\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "#         for neigh in tqdm(connections[node], desc=f\"BFS from node {start_node}\"):\n",
    "        for neigh in connections[node]:\n",
    "            if neigh not in hops:\n",
    "                hops[neigh] = hops[node] + 1\n",
    "                if hops[neigh] > 2:\n",
    "                    continue\n",
    "                queue.append(neigh)\n",
    "                visited.add(neigh)\n",
    "    return visited\n",
    "\n",
    "def extract_subgraphs(connections, label_to_nodes, label_type, count=200):\n",
    "    selected_nodes = list(label_to_nodes[label_type])[:count]\n",
    "    subgraphs = []\n",
    "\n",
    "    for node in tqdm(selected_nodes, desc=f\"Extracting {label_type} subgraphs\"):\n",
    "        neighbors = bfs_2hop(connections, node)\n",
    "\n",
    "        subgraph = []\n",
    "        for neigh in neighbors:\n",
    "            for connected_node in connections[neigh]:\n",
    "                if connected_node in neighbors:\n",
    "                    subgraph.append(f\"{neigh} {connected_node} {labels[neigh]}\")\n",
    "\n",
    "        subgraphs.append(subgraph)\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "def merge_overlapping_subgraphs(subgraphs):\n",
    "    merged_subgraphs = []\n",
    "    visited_nodes = set()\n",
    "\n",
    "    for subgraph in tqdm(subgraphs, desc=\"Merging subgraphs\"):\n",
    "        current_nodes = set([edge.split()[0] for edge in subgraph] + [edge.split()[1] for edge in subgraph])\n",
    "        if not visited_nodes.isdisjoint(current_nodes):\n",
    "            overlap_graph = next((g for g in merged_subgraphs if not set(g.keys()).isdisjoint(current_nodes)), None)\n",
    "            if overlap_graph:\n",
    "                for src, dest, _ in [edge.split() for edge in subgraph]:\n",
    "                    overlap_graph[src].add(dest)\n",
    "            else:\n",
    "                merged_subgraph = defaultdict(set)\n",
    "                for src, dest, _ in [edge.split() for edge in subgraph]:\n",
    "                    merged_subgraph[src].add(dest)\n",
    "                merged_subgraphs.append(merged_subgraph)\n",
    "        else:\n",
    "            new_subgraph = defaultdict(set)\n",
    "            for src, dest, _ in [edge.split() for edge in subgraph]:\n",
    "                new_subgraph[src].add(dest)\n",
    "            merged_subgraphs.append(new_subgraph)\n",
    "\n",
    "        visited_nodes.update(current_nodes)\n",
    "\n",
    "    return merged_subgraphs\n",
    "\n",
    "def save_merged_subgraphs_to_files(merged_subgraphs, output_folder, label_type):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for i, subgraph in enumerate(tqdm(merged_subgraphs, desc=f\"Saving {label_type} merged subgraphs\")):\n",
    "        file_name = os.path.join(output_folder, f\"{label_type}_merged_subgraph_{i}.txt\")\n",
    "        with open(file_name, 'w') as file:\n",
    "            for src, edges in subgraph.items():\n",
    "                for dest in edges:\n",
    "                    file.write(f\"{src} {dest}\\n\")\n",
    "        print(f\"{file_name} saved\")\n",
    "\n",
    "def read_data_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "input_file_path = '/workdir/home/bai/Euni_HO_modified/data/source_data/merged_all.txt'  # Update with your actual file path\n",
    "data_lines = read_data_from_file(input_file_path)\n",
    "connections, labels, label_to_nodes = process_data(data_lines)\n",
    "\n",
    "output_folder = '/workdir/home/bai/Euni_HO_modified/data/source_data/output_graphs(200)'  # Update with your actual output folder path\n",
    "\n",
    "subgraphs_benign = extract_subgraphs(connections, label_to_nodes, 'benign', 200)\n",
    "subgraphs_attack = extract_subgraphs(connections, label_to_nodes, 'attack', 200)\n",
    "\n",
    "merged_subgraphs_benign = merge_overlapping_subgraphs(subgraphs_benign)\n",
    "merged_subgraphs_attack = merge_overlapping_subgraphs(subgraphs_attack)\n",
    "\n",
    "save_merged_subgraphs_to_files(merged_subgraphs_benign, output_folder, 'benign')\n",
    "save_merged_subgraphs_to_files(merged_subgraphs_attack, output_folder, 'attack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d299524e3fd04a678d72e4dc0d723925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving attack merged subgraphs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workdir/home/bai/Euni_HO_modified/data/source_data/output_graphs(200)/attack_merged_subgraph_0.txt saved\n"
     ]
    }
   ],
   "source": [
    "save_merged_subgraphs_to_files(merged_subgraphs_attack, output_folder, 'attack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_data(data_lines):\n",
    "    edges = [tuple(map(int, line.split()[:3])) for line in data_lines]\n",
    "    connections = defaultdict(set)\n",
    "    labels = {}\n",
    "    label_to_nodes = {'benign': set(), 'attack': set()}\n",
    "\n",
    "    for src, dest, label in edges:\n",
    "        connections[src].add(dest)\n",
    "        connections[dest].add(src)\n",
    "        labels[src] = label\n",
    "        labels[dest] = label\n",
    "        if label == 0:\n",
    "            label_to_nodes['benign'].add(src)\n",
    "            label_to_nodes['benign'].add(dest)\n",
    "        elif label == 1:\n",
    "            label_to_nodes['attack'].add(src)\n",
    "            label_to_nodes['attack'].add(dest)\n",
    "\n",
    "    return connections, labels, label_to_nodes\n",
    "\n",
    "def bfs_2hop(connections, start_node):\n",
    "    visited = set()\n",
    "    queue = deque([start_node])\n",
    "    hops = {start_node: 0}\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        for neigh in connections[node]:\n",
    "            if neigh not in hops:\n",
    "                hops[neigh] = hops[node] + 1\n",
    "                if hops[neigh] > 2:\n",
    "                    continue\n",
    "                queue.append(neigh)\n",
    "                visited.add(neigh)\n",
    "    return visited\n",
    "\n",
    "def extract_subgraphs(connections, label_to_nodes, label_type, count=200):\n",
    "    selected_nodes = list(label_to_nodes[label_type])[:count]\n",
    "    subgraphs = []\n",
    "\n",
    "    for node in selected_nodes:\n",
    "        neighbors = bfs_2hop(connections, node)\n",
    "\n",
    "        # Create subgraph\n",
    "        subgraph = []\n",
    "        for neigh in neighbors:\n",
    "            for connected_node in connections[neigh]:\n",
    "                if connected_node in neighbors:\n",
    "                    subgraph.append(f\"{neigh} {connected_node} {labels[neigh]}\")\n",
    "\n",
    "        subgraphs.append(subgraph)\n",
    "\n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merged_subgraphs_to_files(subgraphs, output_folder, label_type):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for i, subgraph in enumerate(tqdm(subgraphs, desc=f\"Saving {label_type} subgraphs to files\")):\n",
    "        file_name = os.path.join(output_folder, f\"{label_type}_subgraph_{i}.txt\")\n",
    "        with open(file_name, 'w') as file:\n",
    "            for edge in subgraph:\n",
    "                file.write(edge + '\\n')\n",
    "        print(f\"{file_name} saved\")\n",
    "\n",
    "def read_data_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "input_file_path = '/workdir/home/bai/Euni_HO_modified/data/3_openKE/synthesize/merged_all.txt'\n",
    "data_lines = read_data_from_file(input_file_path)\n",
    "connections, labels, label_to_nodes = process_data(data_lines)\n",
    "\n",
    "output_folder = '/workdir/home/bai/Euni_HO_modified/data/3_openKE/synthesize/output_graphs'\n",
    "\n",
    "# Extract and save benign subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_subgraphs(subgraphs):\n",
    "    merged_subgraphs = []\n",
    "    visited_nodes = set()\n",
    "\n",
    "    for subgraph in subgraphs:\n",
    "        current_nodes = set([edge[0] for edge in subgraph] + [edge[1] for edge in subgraph])\n",
    "        if not visited_nodes.isdisjoint(current_nodes):\n",
    "            # 如果当前子图与已访问节点有重叠，合并它们\n",
    "            overlap_graph = next((g for g in merged_subgraphs if not set(g.keys()).isdisjoint(current_nodes)), None)\n",
    "            if overlap_graph:\n",
    "                # 更新已合并的子图\n",
    "                for src, dest, label in subgraph:\n",
    "                    overlap_graph[src].add(dest)\n",
    "            else:\n",
    "                # 创建新的合并子图\n",
    "                merged_subgraph = defaultdict(set)\n",
    "                for src, dest, label in subgraph:\n",
    "                    merged_subgraph[src].add(dest)\n",
    "                merged_subgraphs.append(merged_subgraph)\n",
    "        else:\n",
    "            # 创建新的子图\n",
    "            new_subgraph = defaultdict(set)\n",
    "            for src, dest, label in subgraph:\n",
    "                new_subgraph[src].add(dest)\n",
    "            merged_subgraphs.append(new_subgraph)\n",
    "\n",
    "        visited_nodes.update(current_nodes)\n",
    "\n",
    "    return merged_subgraphs\n",
    "\n",
    "# 先使用之前的逻辑提取所有子图\n",
    "subgraphs_benign = extract_subgraphs(connections, label_to_nodes, 'benign', 200)\n",
    "subgraphs_attack = extract_subgraphs(connections, label_to_nodes, 'attack', 200)\n",
    "\n",
    "# 合并重叠的子图\n",
    "merged_subgraphs_benign = merge_overlapping_subgraphs(subgraphs_benign)\n",
    "merged_subgraphs_attack = merge_overlapping_subgraphs(subgraphs_attack)\n",
    "\n",
    "# 保存合并后的子图\n",
    "save_merged_subgraphs_to_files(merged_subgraphs_benign, output_folder, 'benign')\n",
    "save_merged_subgraphs_to_files(merged_subgraphs_attack, output_folder, 'attack')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
